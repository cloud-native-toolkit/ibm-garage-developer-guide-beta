{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Todo This is the existing content from the dev guide front page. Testing Very IBM focussed - is this OK or do we want to be more multi-cloud friendly? The Cloud-Native Toolkit is an open-source collection of assets that enable application development and support teams to deliver business value quickly using Red Hat OpenShift or IBM Cloud-managed Kubernetes. This guide provides information to help Developers, Administrators, and Site Reliability Engineers use the Toolkit to support delivering business applications through the entire Software Development Life Cycle (SDLC). The Cloud-Native Toolkit environment has been built to support the principles of a robust SDLC while being flexible enough to fit into a wide range of development settings and toolchains. The Cloud-Native Toolkit supports different tool selections, from open source versions of tools like Artifactory and SonarQube to enterprise-class software like IBM Cloud Pak for Applications and IBM Multicloud Manager.","title":"Home"},{"location":"adopting/admin.html","text":"Toolkit Administration Guide \u00b6 Todo Complete this content from here This content should contain admin functions. And reference type content should move to the reference section, any additonal content related to underlying platforms should be in the resources section Todo What content is relevant? CodeReady Workspaces? Dashboard config Artifactory setup Sysdig setup Terraform modules - move to reference section Cluster configuration - mostry reference content Destroying","title":"Toolkit administration"},{"location":"adopting/admin.html#toolkit-administration-guide","text":"Todo Complete this content from here This content should contain admin functions. And reference type content should move to the reference section, any additonal content related to underlying platforms should be in the resources section Todo What content is relevant? CodeReady Workspaces? Dashboard config Artifactory setup Sysdig setup Terraform modules - move to reference section Cluster configuration - mostry reference content Destroying","title":"Toolkit Administration Guide"},{"location":"adopting/adopting.html","text":"Adopting the Cloud-Native Toolkit \u00b6 After you complete the learning section you may want to install the toolkit in your working environment and start adopting the Toolkit. This section provides information about some alternate install options for the Toolkit, how to customize the toolkit and some best practices for using the Toolkit in various use cases. Todo Provide a brief introduction to the sections within Adopting the Toolkit: Setup options Customizing the toolkit Admin guide Best practices Use cases","title":"Adopting the Toolkit"},{"location":"adopting/adopting.html#adopting-the-cloud-native-toolkit","text":"After you complete the learning section you may want to install the toolkit in your working environment and start adopting the Toolkit. This section provides information about some alternate install options for the Toolkit, how to customize the toolkit and some best practices for using the Toolkit in various use cases. Todo Provide a brief introduction to the sections within Adopting the Toolkit: Setup options Customizing the toolkit Admin guide Best practices Use cases","title":"Adopting the Cloud-Native Toolkit"},{"location":"adopting/applying.html","text":"Use Cases \u00b6 Create an operator : Shows how the toolkit can be used to create an operator Quarkus : - https://cloudnativereference.dev/deployments/quarkus - https://github.com/ibm-garage-cloud/planning/issues/705","title":"User cases"},{"location":"adopting/applying.html#use-cases","text":"Create an operator : Shows how the toolkit can be used to create an operator Quarkus : - https://cloudnativereference.dev/deployments/quarkus - https://github.com/ibm-garage-cloud/planning/issues/705","title":"Use Cases"},{"location":"adopting/audit-logging.html","text":"","title":"Auditing and Logging"},{"location":"adopting/best-practices.html","text":"Adopting the Cloud-Native Toolkit \u00b6 Todo Complete the summary of the installation and configuration options available","title":"Toolkit best practices"},{"location":"adopting/best-practices.html#adopting-the-cloud-native-toolkit","text":"Todo Complete the summary of the installation and configuration options available","title":"Adopting the Cloud-Native Toolkit"},{"location":"adopting/customize.html","text":"Customizing the Toolkit installation \u00b6 Todo How do we reconcile all the options for installing in different environments, the different install methods and the desired outcomes? Proposal: Fast start creates a standard install, suitable for working through the learning This section talks about options to move away from the standard install switch components (e.g. Jenkins instead of Tekton/OpenShift Pipelines) platform specific components (e.g. IBM Cloud Pak) Reference section contains the detail of each option Components supported by the toolkit. How they function within the toolkit ConfigMaps, Secrets, CRD created for the component Any specific customizations, config the toolkit adds for enable a component Tekton modules available and details of what each module does Pipelines and Tasks installed and details of what each module does","title":"Customizing the Toolkit"},{"location":"adopting/customize.html#customizing-the-toolkit-installation","text":"Todo How do we reconcile all the options for installing in different environments, the different install methods and the desired outcomes? Proposal: Fast start creates a standard install, suitable for working through the learning This section talks about options to move away from the standard install switch components (e.g. Jenkins instead of Tekton/OpenShift Pipelines) platform specific components (e.g. IBM Cloud Pak) Reference section contains the detail of each option Components supported by the toolkit. How they function within the toolkit ConfigMaps, Secrets, CRD created for the component Any specific customizations, config the toolkit adds for enable a component Tekton modules available and details of what each module does Pipelines and Tasks installed and details of what each module does","title":"Customizing the Toolkit installation"},{"location":"adopting/security.html","text":"","title":"Security and Permissions"},{"location":"adopting/operator/operator.html","text":"Building an operator \u00b6 Set up DevOps pipelines to build an operator and operator catalog The Cloud-Native Toolkit provides Tekton pipelines to automate the process to build and deploy operator-based applications. The operator pipeline supports Ansible, Go, and Helm-based operators that have been built using the Operator SDK. The Cloud-Native Toolkit also provides a companion Tekton pipeline to build a custom Operator Lifecycle Manager catalog that simplifies deployment of the operators to a cluster. The following steps will walk you through using the pipelines to build an operator and operator catalog and deploy them to a cluster. Prerequisites \u00b6 Operator SDK \u00b6 The Tekton pipeline for building operators requires that the operator has been created by at least v1.0.0 of the Operator SDK but we recommend using the latest available (v1.3.0 at the time of this writing). The Operator SDK structure had a major architectural change with v1.0.0 and continues to have significant improvements made with each release. If you do not have operator-sdk , follow the installation instructions to get it. (Optional) Create your operator \u00b6 These steps will walk you through building a simple operator following the Operator SDK quick-start guide. You can choose between Ansible , Go , or Helm quick start guides for the following example. Note If you already have an operator built with v1.0.0+ of the Operator SDK then you can skip to the next section. The Ansible operator quick start seems to be the simplest way to get up and running quickly and exercise the Tekton pipelines. We've included the abbreviated steps from the Ansible quick start guide below. Create a directory for your operator and change to that directory mkdir memcached-operator cd memcached-operator Initialize the operator operator-sdk init --plugins = ansible --domain = example.com where: --plugins can be either ansible , go , or helm . If --plugins is not provided then it defaults to go --domain is the base domain name used for the Custom Resource Definition generated for the operator Create an API for the operator operator-sdk create api --group cache --version v1 --kind Memcached --generate-role where: - --group is the api group for the CRD - --kind is the resource kind for the CRD - --version is the resource version for the CRD Generate metadata for Operator bundle make bundle Answer the interactive questions Create the README.md and initialize git repository echo \"# memcached operator\" > README.md git init git add . git commit -m \"Initial commit\" Create a new remote git repository on a git server such as GitHub and copy the url Push the local git repository to the remote GitHub repository git remote add origin ${ GIT_URL } git push -u origin $( git rev-parse --abbrev-ref HEAD ) At this point you have enough of a functioning operator to try out the Tekton operator pipeline. If you want to explore the operator you have created and/or enhance its capabilities, there are more detailed instructions on the operator sdk page. Register the operator pipeline \u00b6 This step is similar to deploying an application . 1. Log into your Development Cluster from the command line \u00b6 Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Info Todo Fix the workspaces link, once that content has been migrated If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding 2. Create the operator development namespace \u00b6 Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ OPERATOR_NAMESPACE } 3. Register the operator in a DevOps Pipeline \u00b6 Start the process to create a pipeline using Tekton. oc pipeline --tekton The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token The CLI will attempt to determine the runtime of your repository. It should detect that it is an operator and prompt you to select ibm-operator pipeline or ibm-operator-catalog pipeline. Select ibm-operator pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No). Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console. 4. View your application pipeline \u00b6 Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Register the operator catalog pipeline \u00b6 The operator catalog repository is a simple repo used to keep track of the operator bundles that will be packaged together in the catalog. Even though you may start with one operator, it is assumed that ultimately multiple operators will be managed together in one catalog. Open the operator catalog template repository and create a repository for the operator catalog using the template Copy the repository url Create/connect to the project for the operator development. This does not need to be the same one used for the operator build. oc sync ${ OPERATOR_NAMESPACE } Register the pipeline for the operator catalog oc pipeline --tekton ${ GIT_URL } Register the catalog repository with the operator pipeline \u00b6 The last step of the operator pipeline will update the operator catalog repository with the new version of the operator bundle that was created. This update will trigger the creation of a new version of the catalog. Change to the project where the operator pipeline was registered oc project ${ OPERATOR_NAMESPACE } Change the directory to the local clone of your operator catalog repository Register the url for the operator catalog repository for the operator pipeline igc git-secret olm-catalog-repo Kick off the operator pipeline again to update the operator catalog repo Configure the GitOps repo \u00b6 If you don't already have one, create a GitOps repo from the gitops template template Clone the GitOps repo to the local machine git clone ${ GIT_URL } Change the directory to the cloned GitOps repo Change to the project where the operator catalog pipeline was registered oc project ${ OPERATOR_NAMESPACE } Register the GitOps repo url oc gitops Trigger the operator catalog pipeline build to register the catalog source in the GitOps repo Configure ArgoCD to deploy the CatalogSource into the namespace openshift-marketplace","title":"Build an Operator"},{"location":"adopting/operator/operator.html#building-an-operator","text":"Set up DevOps pipelines to build an operator and operator catalog The Cloud-Native Toolkit provides Tekton pipelines to automate the process to build and deploy operator-based applications. The operator pipeline supports Ansible, Go, and Helm-based operators that have been built using the Operator SDK. The Cloud-Native Toolkit also provides a companion Tekton pipeline to build a custom Operator Lifecycle Manager catalog that simplifies deployment of the operators to a cluster. The following steps will walk you through using the pipelines to build an operator and operator catalog and deploy them to a cluster.","title":"Building an operator"},{"location":"adopting/operator/operator.html#prerequisites","text":"","title":"Prerequisites"},{"location":"adopting/operator/operator.html#operator-sdk","text":"The Tekton pipeline for building operators requires that the operator has been created by at least v1.0.0 of the Operator SDK but we recommend using the latest available (v1.3.0 at the time of this writing). The Operator SDK structure had a major architectural change with v1.0.0 and continues to have significant improvements made with each release. If you do not have operator-sdk , follow the installation instructions to get it.","title":"Operator SDK"},{"location":"adopting/operator/operator.html#optional-create-your-operator","text":"These steps will walk you through building a simple operator following the Operator SDK quick-start guide. You can choose between Ansible , Go , or Helm quick start guides for the following example. Note If you already have an operator built with v1.0.0+ of the Operator SDK then you can skip to the next section. The Ansible operator quick start seems to be the simplest way to get up and running quickly and exercise the Tekton pipelines. We've included the abbreviated steps from the Ansible quick start guide below. Create a directory for your operator and change to that directory mkdir memcached-operator cd memcached-operator Initialize the operator operator-sdk init --plugins = ansible --domain = example.com where: --plugins can be either ansible , go , or helm . If --plugins is not provided then it defaults to go --domain is the base domain name used for the Custom Resource Definition generated for the operator Create an API for the operator operator-sdk create api --group cache --version v1 --kind Memcached --generate-role where: - --group is the api group for the CRD - --kind is the resource kind for the CRD - --version is the resource version for the CRD Generate metadata for Operator bundle make bundle Answer the interactive questions Create the README.md and initialize git repository echo \"# memcached operator\" > README.md git init git add . git commit -m \"Initial commit\" Create a new remote git repository on a git server such as GitHub and copy the url Push the local git repository to the remote GitHub repository git remote add origin ${ GIT_URL } git push -u origin $( git rev-parse --abbrev-ref HEAD ) At this point you have enough of a functioning operator to try out the Tekton operator pipeline. If you want to explore the operator you have created and/or enhance its capabilities, there are more detailed instructions on the operator sdk page.","title":"(Optional) Create your operator"},{"location":"adopting/operator/operator.html#register-the-operator-pipeline","text":"This step is similar to deploying an application .","title":"Register the operator pipeline"},{"location":"adopting/operator/operator.html#1-log-into-your-development-cluster-from-the-command-line","text":"Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Info Todo Fix the workspaces link, once that content has been migrated If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding","title":"1. Log into your Development Cluster from the command line"},{"location":"adopting/operator/operator.html#2-create-the-operator-development-namespace","text":"Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ OPERATOR_NAMESPACE }","title":"2. Create the operator development namespace"},{"location":"adopting/operator/operator.html#3-register-the-operator-in-a-devops-pipeline","text":"Start the process to create a pipeline using Tekton. oc pipeline --tekton The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token The CLI will attempt to determine the runtime of your repository. It should detect that it is an operator and prompt you to select ibm-operator pipeline or ibm-operator-catalog pipeline. Select ibm-operator pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No). Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console.","title":"3. Register the operator in a DevOps Pipeline"},{"location":"adopting/operator/operator.html#4-view-your-application-pipeline","text":"Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below.","title":"4. View your application pipeline"},{"location":"adopting/operator/operator.html#register-the-operator-catalog-pipeline","text":"The operator catalog repository is a simple repo used to keep track of the operator bundles that will be packaged together in the catalog. Even though you may start with one operator, it is assumed that ultimately multiple operators will be managed together in one catalog. Open the operator catalog template repository and create a repository for the operator catalog using the template Copy the repository url Create/connect to the project for the operator development. This does not need to be the same one used for the operator build. oc sync ${ OPERATOR_NAMESPACE } Register the pipeline for the operator catalog oc pipeline --tekton ${ GIT_URL }","title":"Register the operator catalog pipeline"},{"location":"adopting/operator/operator.html#register-the-catalog-repository-with-the-operator-pipeline","text":"The last step of the operator pipeline will update the operator catalog repository with the new version of the operator bundle that was created. This update will trigger the creation of a new version of the catalog. Change to the project where the operator pipeline was registered oc project ${ OPERATOR_NAMESPACE } Change the directory to the local clone of your operator catalog repository Register the url for the operator catalog repository for the operator pipeline igc git-secret olm-catalog-repo Kick off the operator pipeline again to update the operator catalog repo","title":"Register the catalog repository with the operator pipeline"},{"location":"adopting/operator/operator.html#configure-the-gitops-repo","text":"If you don't already have one, create a GitOps repo from the gitops template template Clone the GitOps repo to the local machine git clone ${ GIT_URL } Change the directory to the cloned GitOps repo Change to the project where the operator catalog pipeline was registered oc project ${ OPERATOR_NAMESPACE } Register the GitOps repo url oc gitops Trigger the operator catalog pipeline build to register the catalog source in the GitOps repo Configure ArgoCD to deploy the CatalogSource into the namespace openshift-marketplace","title":"Configure the GitOps repo"},{"location":"adopting/setup/air-gapped-setup.html","text":"Todo Create this content","title":"Air-gapped setup"},{"location":"adopting/setup/firewalled-setup.html","text":"Todo Create this content","title":"Firewalled setup"},{"location":"adopting/setup/ibmcloud-setup.html","text":"Configuring IBM Cloud based setup \u00b6 Todo migrate this content from here post-installation - integration LogDNA and SysDig","title":"Configuring IBM Cloud based setup"},{"location":"adopting/setup/ibmcloud-setup.html#configuring-ibm-cloud-based-setup","text":"Todo migrate this content from here post-installation - integration LogDNA and SysDig","title":"Configuring IBM Cloud based setup"},{"location":"adopting/setup/installing.html","text":"Installing the Cloud-Native Toolkit \u00b6 Todo Complete this content What are the setup options we support? OpenShift Private catalog Iteration Zero - customized setup Fast Start (already covered in Install section) IKS Private catalog Iteration Zero No cost options? OpenLabs? OKD? CRC? minikube / Docker Kube / microK8s? Initially the Fast-start installation method is the recommended way to install the Cloud-Native Toolkit, as this is largely an automatic install with minimal choices required and creates a default configuration suitable for learning about cloud native development. When you want to adopt cloud native development within an enterprise development team, then you may want more control over the installation and configuration of the toolkit. This section outlines more advanced options available for installing the toolkit. IBM Cloud Private Catalog: This provides an easy mechanism to easily create and delete instances of the Cloud-Native Toolkit on OpenShift clusters within the IBM Cloud. Iteration-Zero: This gives you full control over customizing how the toolkit is installed and what components you want deployed as part of the toolkit","title":"Installation options"},{"location":"adopting/setup/installing.html#installing-the-cloud-native-toolkit","text":"Todo Complete this content What are the setup options we support? OpenShift Private catalog Iteration Zero - customized setup Fast Start (already covered in Install section) IKS Private catalog Iteration Zero No cost options? OpenLabs? OKD? CRC? minikube / Docker Kube / microK8s? Initially the Fast-start installation method is the recommended way to install the Cloud-Native Toolkit, as this is largely an automatic install with minimal choices required and creates a default configuration suitable for learning about cloud native development. When you want to adopt cloud native development within an enterprise development team, then you may want more control over the installation and configuration of the toolkit. This section outlines more advanced options available for installing the toolkit. IBM Cloud Private Catalog: This provides an easy mechanism to easily create and delete instances of the Cloud-Native Toolkit on OpenShift clusters within the IBM Cloud. Iteration-Zero: This gives you full control over customizing how the toolkit is installed and what components you want deployed as part of the toolkit","title":"Installing the Cloud-Native Toolkit"},{"location":"contribute/automation.html","text":"","title":"Build Automation"},{"location":"contribute/contribute.html","text":"Contributing to the project \u00b6 Cloud-Native Toolkit \u00b6 Cloud-Native Toolkit is an organized effort managed by the IBM Garage to help development teams build and deliver cloud-native solutions deployed to IBM Cloud with IBM Kubernetes and Red Hat OpenShift Managed Service. The work is managed as an Open Source project and contribution is welcome from anyone that would like to participate. The following guidelines will help everyone navigate the process of contributing. (Most of these steps will generally apply to any Open Source project.) Project structure \u00b6 Before diving into the steps, it's important to get a general understanding of the organization structure of the repositories. The various components that make up the Cloud-Native Toolkit are organized into different git repositories that are aligned around solution space, release schedule, and intended audience. Currently, these repositories are divided between two public Git organizations: github.com/ibm-garage-cloud github.com/IBM All of the repositories in the github.com/ibm-garage-cloud organization belong to the Cloud-Native Toolkit. Within the github.com/IBM organization only the following repositories contain Cloud-Native Toolkit content: IBM/template-graphql-typescript IBM/template-java-spring IBM/template-node-angular IBM/template-node-react IBM/template-node-typescript IBM/ibm-garage-tekton-tasks IBM/template-argocd-gitops IBM/template-ibmcloud-services (These repositories have been relocated to the github.com/IBM because their content applies more broadly than the Cloud-Native Toolkit and/or fits in with the larger catalog of related Code Pattern and Starter Kit content). How to contribute \u00b6 1. Look through the issues \u00b6 Info Prerequisite: We use ZenHub to add metadata to the Git issues so they can be managed via a Kanban board. ZenHub also allows us to show the issues and pull requests from the various repositories in on place. Viewing the issues with ZenHub requires a Chrome extension . We highly recommend installing this extension to help search the issues. Whether you have a question/issue with existing behavior or an idea for a new feature that can be added to the Toolkit, the place to begin is looking through the issues. Each repository has its own list of issues. If you have an issue with content from a specific repository then any related issues will likely be found there. However, given the wide range of repositories and the interconnected nature of many of the components it is usually best to look at the ZenHub board. The following steps are the best way to view the board: Open the github.com/ibm-garage-cloud/planning repository Click on the ZenHub link along the top menu. Note: You must install the ZenHub extension for the link to appear Look at the Repos drop-down in the top-right of the board to ensure all the issues for the various Cloud-Native Toolkit repositories have been displayed. If the drop-down does not read Repos (24/24) then do the following to select them all: Click on the Repos drop-down Select the Show all repos link Use the filters and/or the search bar to find cards related to your issue. If you find an existing ticket that is related to your issue, add your information to the card. Existing defect \u00b6 If you find an existing defect, be sure to add a comment with the details of your specific scenario. When the defect has been addressed and the ticket has been moved to the Review/QA stage, we will ask you to help test the solution in your environment. Existing feature \u00b6 If you find an existing feature request, please indicate your interest through a comment. As appropriate, include the business requirements or user story that is driving the request. We will use this input to help determine prioritization of new features. No issue found \u00b6 If you are unable to find a card that is related to your issue or feature, proceed to the next step to create a new issue. The search doesn't need to be an exhaustive one and if there is any question whether the item is new or a duplicate, go ahead and create a new issue. We'd rather have the item captured and mark it as a duplicate after the fact, if necessary, than to have an issue fall through the cracks. 2. Create a new issue \u00b6 If you could not find an existing issue related to your problem or feature then its time to create a new issue. Issues fall in one of two categories: Bug Feature Bug \u00b6 Bugs can be either be reported against the repository that has the problem or in the general github.com/ibm-garage-cloud/planning repository. These are the steps for reporting a bug: Navigate to the repository in the browser Click on the Issues menu item at the top then click on New Issue Click Get Started on the Bug report template to create a new issue from the template. Note: At the moment, some repositories do not have Bug report templates defined. If a template does not exist you will see the blank issue dialog. Provide all the information relevant to the bug, particularly the details of your scenario and the steps to reproduce Feature \u00b6 Features should be reported against the github.com/ibm-garage-cloud/planning repository. These are the steps for requesting a feature: Navigate to the github.com/ibm-garage-cloud/planning repository in the browser Click on the Issues menu item at the top then click on New Issue Click Get Started on the Feature request template to create a new issue from the template Provide all the information relevant to the bug, particularly the details of the problem addressed by the feature and the impact/benefit of implementing the feature 3. Setup your repository for development \u00b6 On an Open Source project there are typically a few maintainers who have direct access to the repositories and a larger number of contributors who do not. In this case, the way to introduce changes to the repositories is through a fork. The process for setting up the fork is as follows: Note Even though the maintainers have direct access to the repositories, we follow this same process of working off of a fork of the upstream repository as a best practice. Clone the repository to which you want to introduce changes (the upstream repository) to your local machine git clone { upstream repo url } Create a fork of the upstream repository by pressing the Fork button at the top of the page Copy the url of the repository fork Open a terminal to the cloned directory and run the following to set the push url to be the url of the repository fork git remote set-url --push origin { fork repo url } List the remotes for the local repository clone to verify that the fetch and push urls are pointing to the appropriate repositories git remote -v The output should look something like the following, with the fetch pointing to the upstream url and the push pointing to the fork With the local clone set up this way, any time you get changes from the remote (e.g. git pull ) the changes will be pulled from the upstream repository. Similarly, when you push changes they will be pushed to the fork . 4. Develop your changes \u00b6 Now that you have a fork and a local clone of the repository, you can start making your changes. This part is mostly business-as-usual for software development. We have a couple of best practices we recommend: Work in a branch \u00b6 It is a best practice to make your changes in a separate branch, even though the changes will be made in your own fork. There are at least two good reasons for doing this: The branch can be named after the issue number and the feature Naming the branch according to the change that is being made provides a bit of documentation for the purpose of the branch. It also helps enforce the notion that the branch exists only for the implementation of that feature. The branch can be rebased when new changes come in from the upstream Through the course of development of the branch, other changes may be introduced in the upstream repository. Making the changes in a separate branch allows for the upstream changes to be easily pulled in on the master branch and applied to other branches as appropriate. Create the branch by running: git checkout -b { branch name } Push the branch up to your fork by running: git push -u origin { branch name } Commit message format \u00b6 Each commit message should clearly describe the changes that are being made. During the development process as many small changes are made, a single one-liner is sufficient for the commit message. With larger changes or when the changes in the branch are squashed into a single commit, the following commit message format is preferred. Writing commit messages <type> indicates the type of commit that\u2019s being made. This can be: feat , fix , perf , docs , chore , style , refactor . <scope> the scope could be anything specifying place of the commit change or the thing(s) that changed. <subject> the subject should be a short overview of the change. <body> the body should be a detailed description of the change(s). This can be prose or a bulleted listing. <issue reference> the issue reference should be a reference to the issue number under which this change is being made. The issue reference should be in the format of {git org}/{git repo}#{issue number} Commit message format: type(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <issue reference> Create a draft pull request when the branch is first pushed to the fork \u00b6 GitHub recently introduced draft pull requests that allow a pull request to be recorded but marked as not yet ready for review. Git provides a url to open a pull request the first time a branch is first pushed to the remote, which gives an excellent and easy opportunity to create the draft. Note Be sure to link the pull request with the issue Creating a draft pull request early has the following benefits: Clicking the link provided by Git sets up the source and target repos/branches for you so you don't need to hunt around Having the draft pull request gives insight for everyone else where the work is being done Push changes to your fork frequently during development \u00b6 As you are making changes, push them frequently to the fork. This ensures that your code is backed up somewhere and allows everyone else to see what activity is happening. It also means that if you get pulled into some other work, the latest version of your changes are available for others to possibly pick up where you left off. Pull in the latest changes from master frequently and rebase your branch \u00b6 It is good to make sure you are always working off of the latest code from the upstream. With the changes in a separate branch, it is easy to bring in upstream changes with the following steps: Checkout master in the local clone git checkout master Pull in the changes from the upstream repository git pull Check out your branch git checkout { branch } Rebase your branch on master git rebase master Force push the changes git push --force-with-lease 5. Create your pull request \u00b6 GitHub has recently added a new feature that allows a pull request to put into draft status. This is helpful to record a pull request as pending work even if the changes are not yet ready for review. Open your fork repository in a browser Click New pull request Select the appropriate target and source branches for the pull request and press Create pull request base repository is the target repository into which your changes should be merged (should be the upstream repository) base is the target branch in the upstream repository into which your changes should be merged (typically this will be master ) head repository is the source of the changes (this should be your fork repository) compare is the branch containing your changes that should be merged Write the title and description of the pull request How to write the perfect pull request Link the pull request to the related issue Click Create pull request or Create draft pull request to submit Note A pull request can be converted to a draft after it was created by clicking on the Convert to draft link located unter the Reviewers section on the right-hand side. 6. Prepare your branch to submit the pull request for review \u00b6 Pull in the latest changes from master frequently and rebase your branch, as described in the previous section (Optionally) Rebase your branch to squash commits and clean up commit messages. An interactive rebase will allow you to clean up your branch before submitting it for review. This will reduce the number of commits down to the core set of changes that reflect the feature/bug fix, remove any commits that aren't part of the change you are making, and clean up the commit messages to clearly describe the changes and follow the commit message format guidelines. 7. Submit your pull request for review \u00b6 Assuming you have previously created a draft pull request, when you are ready to have your code reviewed and merged then you will need to indicate that in the pull request. Open the browser to the upstream repository. Select the Pull requests tab, find your pull request in the list and open it. Press the Ready for review button to tell the maintainers the pull request is ready to be processed. 8. Update your pull request \u00b6 Keep an eye on the pull request after it has been submitted for review. The maintainers may have questions or request changes before the pull request can be closed. The GitHub system should notify you when changes are made to your pull request. Also, the maintainers all have day jobs and sometimes pull requests get overlooked. If your pull request has sat for a while you can get some attention to it by tagging one of the maintainers in a commit comment. E.g. @seansund @csantana @bwoolf1 @lsteck","title":"Contribute"},{"location":"contribute/contribute.html#contributing-to-the-project","text":"","title":"Contributing to the project"},{"location":"contribute/contribute.html#cloud-native-toolkit","text":"Cloud-Native Toolkit is an organized effort managed by the IBM Garage to help development teams build and deliver cloud-native solutions deployed to IBM Cloud with IBM Kubernetes and Red Hat OpenShift Managed Service. The work is managed as an Open Source project and contribution is welcome from anyone that would like to participate. The following guidelines will help everyone navigate the process of contributing. (Most of these steps will generally apply to any Open Source project.)","title":"Cloud-Native Toolkit"},{"location":"contribute/contribute.html#project-structure","text":"Before diving into the steps, it's important to get a general understanding of the organization structure of the repositories. The various components that make up the Cloud-Native Toolkit are organized into different git repositories that are aligned around solution space, release schedule, and intended audience. Currently, these repositories are divided between two public Git organizations: github.com/ibm-garage-cloud github.com/IBM All of the repositories in the github.com/ibm-garage-cloud organization belong to the Cloud-Native Toolkit. Within the github.com/IBM organization only the following repositories contain Cloud-Native Toolkit content: IBM/template-graphql-typescript IBM/template-java-spring IBM/template-node-angular IBM/template-node-react IBM/template-node-typescript IBM/ibm-garage-tekton-tasks IBM/template-argocd-gitops IBM/template-ibmcloud-services (These repositories have been relocated to the github.com/IBM because their content applies more broadly than the Cloud-Native Toolkit and/or fits in with the larger catalog of related Code Pattern and Starter Kit content).","title":"Project structure"},{"location":"contribute/contribute.html#how-to-contribute","text":"","title":"How to contribute"},{"location":"contribute/contribute.html#1-look-through-the-issues","text":"Info Prerequisite: We use ZenHub to add metadata to the Git issues so they can be managed via a Kanban board. ZenHub also allows us to show the issues and pull requests from the various repositories in on place. Viewing the issues with ZenHub requires a Chrome extension . We highly recommend installing this extension to help search the issues. Whether you have a question/issue with existing behavior or an idea for a new feature that can be added to the Toolkit, the place to begin is looking through the issues. Each repository has its own list of issues. If you have an issue with content from a specific repository then any related issues will likely be found there. However, given the wide range of repositories and the interconnected nature of many of the components it is usually best to look at the ZenHub board. The following steps are the best way to view the board: Open the github.com/ibm-garage-cloud/planning repository Click on the ZenHub link along the top menu. Note: You must install the ZenHub extension for the link to appear Look at the Repos drop-down in the top-right of the board to ensure all the issues for the various Cloud-Native Toolkit repositories have been displayed. If the drop-down does not read Repos (24/24) then do the following to select them all: Click on the Repos drop-down Select the Show all repos link Use the filters and/or the search bar to find cards related to your issue. If you find an existing ticket that is related to your issue, add your information to the card.","title":"1. Look through the issues"},{"location":"contribute/contribute.html#2-create-a-new-issue","text":"If you could not find an existing issue related to your problem or feature then its time to create a new issue. Issues fall in one of two categories: Bug Feature","title":"2. Create a new issue"},{"location":"contribute/contribute.html#3-setup-your-repository-for-development","text":"On an Open Source project there are typically a few maintainers who have direct access to the repositories and a larger number of contributors who do not. In this case, the way to introduce changes to the repositories is through a fork. The process for setting up the fork is as follows: Note Even though the maintainers have direct access to the repositories, we follow this same process of working off of a fork of the upstream repository as a best practice. Clone the repository to which you want to introduce changes (the upstream repository) to your local machine git clone { upstream repo url } Create a fork of the upstream repository by pressing the Fork button at the top of the page Copy the url of the repository fork Open a terminal to the cloned directory and run the following to set the push url to be the url of the repository fork git remote set-url --push origin { fork repo url } List the remotes for the local repository clone to verify that the fetch and push urls are pointing to the appropriate repositories git remote -v The output should look something like the following, with the fetch pointing to the upstream url and the push pointing to the fork With the local clone set up this way, any time you get changes from the remote (e.g. git pull ) the changes will be pulled from the upstream repository. Similarly, when you push changes they will be pushed to the fork .","title":"3. Setup your repository for development"},{"location":"contribute/contribute.html#4-develop-your-changes","text":"Now that you have a fork and a local clone of the repository, you can start making your changes. This part is mostly business-as-usual for software development. We have a couple of best practices we recommend:","title":"4. Develop your changes"},{"location":"contribute/contribute.html#5-create-your-pull-request","text":"GitHub has recently added a new feature that allows a pull request to put into draft status. This is helpful to record a pull request as pending work even if the changes are not yet ready for review. Open your fork repository in a browser Click New pull request Select the appropriate target and source branches for the pull request and press Create pull request base repository is the target repository into which your changes should be merged (should be the upstream repository) base is the target branch in the upstream repository into which your changes should be merged (typically this will be master ) head repository is the source of the changes (this should be your fork repository) compare is the branch containing your changes that should be merged Write the title and description of the pull request How to write the perfect pull request Link the pull request to the related issue Click Create pull request or Create draft pull request to submit Note A pull request can be converted to a draft after it was created by clicking on the Convert to draft link located unter the Reviewers section on the right-hand side.","title":"5. Create your pull request"},{"location":"contribute/contribute.html#6-prepare-your-branch-to-submit-the-pull-request-for-review","text":"Pull in the latest changes from master frequently and rebase your branch, as described in the previous section (Optionally) Rebase your branch to squash commits and clean up commit messages. An interactive rebase will allow you to clean up your branch before submitting it for review. This will reduce the number of commits down to the core set of changes that reflect the feature/bug fix, remove any commits that aren't part of the change you are making, and clean up the commit messages to clearly describe the changes and follow the commit message format guidelines.","title":"6. Prepare your branch to submit the pull request for review"},{"location":"contribute/contribute.html#7-submit-your-pull-request-for-review","text":"Assuming you have previously created a draft pull request, when you are ready to have your code reviewed and merged then you will need to indicate that in the pull request. Open the browser to the upstream repository. Select the Pull requests tab, find your pull request in the list and open it. Press the Ready for review button to tell the maintainers the pull request is ready to be processed.","title":"7. Submit your pull request for review"},{"location":"contribute/contribute.html#8-update-your-pull-request","text":"Keep an eye on the pull request after it has been submitted for review. The maintainers may have questions or request changes before the pull request can be closed. The GitHub system should notify you when changes are made to your pull request. Also, the maintainers all have day jobs and sometimes pull requests get overlooked. If your pull request has sat for a while you can get some attention to it by tagging one of the maintainers in a commit comment. E.g. @seansund @csantana @bwoolf1 @lsteck","title":"8. Update your pull request"},{"location":"contribute/documentation.html","text":"Updating the Developer Guide \u00b6 The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server. Document layout \u00b6 The documentation is organized into distinct sections to provide easy navigation and allow self-service viewers to get started, then dive into deeper content with a clear path. The sections are as follows: Overview - This is high level information about the Cloud-Native Toolkit. What it is and why does it exist. Install - This is a section designed to help the first time user get up and running with minimal knowledge. More advanced install options are provided in a later section Learning - This section is designed to teach the fundamentals of Cloud-Native development with the toolkit. Using a default installation of the Toolkit to work through Continuous Integration and Continuous Delivery. This section is more about Cloud-Native development and how the Toolkit provides functionality to support Cloud-Natove development rather than a deep dive on specific tools that are part of the toolkit. Adopting - This section is designed to move from the initial learning phase to adopting the Toolkit as part of development activities. It covers more advanced installation options, customization options, best practices and how the Toolkit can be applied to certain use cases Reference - The reference section is the technical documentation for the resources delivered by the Toolkit. Resources - The resources section provides links to content outside the toolkit that someone learning the toolkit may find usefu Contributing - This section provides how someone can become a contributor to the Cloud-Native Toolkit project, which includes the core Toolkit, adding additional starter kits or pipeline tasks, updating or adding to the documentation. Creating content \u00b6 MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page. Standard Markdown features \u00b6 The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additiona; # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions. Links within MkDocs generated content \u00b6 MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognised as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file. When the site is generated the filename will be automatically converted to the correct URL Extensions used in the prototype \u00b6 There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here Link configuration \u00b6 Links on the page or embedded images can be annotated to control the links and also the appearance of the links: Image \u00b6 Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center } External Links \u00b6 External links can also use attribute lists to control behaviours, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } YouTube videos \u00b6 It is not possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML to embed a video: < iframe width = \"560\" height = \"315\" src = \"https://www.youtube.com/embed/u3PTRqkd94k\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe > but you can use a link that will open the video in a new browser tab using pure markdown: [ ![alt-content ]( img URL )](video URL \"video title\"){: target=\"_blank\"} where: alt-content is the content used if the image cannot be displayed by the browser img URL is the URL for the thumbnail image for the video. For YouTube this is usually http://img.youtube.com/vi/<video ID>/0.jpg video URL is the URL used to launch the video in a new tab (this can be found by opening the video on YouTube and capturing the URL from the browser address bar - this also provides the video ID to be used in the image). It typically looks like https://youtu.be/u3PTRqkd94k or https://www.youtube.com/watch?v=u3PTRqkd94k video title is a string that is displayed to the viewer when they hover over the video image Info You can also use the same technique to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank} Tabs \u00b6 Content can be organised into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World Information boxes \u00b6 The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note Supported Admonition Classes \u00b6 The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote Setting up an documentation environment \u00b6 To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages `pip install -r requirements.txt' You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000. Everytime a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. Warning Within the configuration mkdocs is configured to fail with internal broken links. This will also kill the local test server. If you want to disable this functionality change the strict value in mkdocs.yml from strict: true to strict: false . Do not check in mkdocs.yaml with strict disabled, as we don't want to publish broken documentation!","title":"Documentation"},{"location":"contribute/documentation.html#updating-the-developer-guide","text":"The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server.","title":"Updating the Developer Guide"},{"location":"contribute/documentation.html#document-layout","text":"The documentation is organized into distinct sections to provide easy navigation and allow self-service viewers to get started, then dive into deeper content with a clear path. The sections are as follows: Overview - This is high level information about the Cloud-Native Toolkit. What it is and why does it exist. Install - This is a section designed to help the first time user get up and running with minimal knowledge. More advanced install options are provided in a later section Learning - This section is designed to teach the fundamentals of Cloud-Native development with the toolkit. Using a default installation of the Toolkit to work through Continuous Integration and Continuous Delivery. This section is more about Cloud-Native development and how the Toolkit provides functionality to support Cloud-Natove development rather than a deep dive on specific tools that are part of the toolkit. Adopting - This section is designed to move from the initial learning phase to adopting the Toolkit as part of development activities. It covers more advanced installation options, customization options, best practices and how the Toolkit can be applied to certain use cases Reference - The reference section is the technical documentation for the resources delivered by the Toolkit. Resources - The resources section provides links to content outside the toolkit that someone learning the toolkit may find usefu Contributing - This section provides how someone can become a contributor to the Cloud-Native Toolkit project, which includes the core Toolkit, adding additional starter kits or pipeline tasks, updating or adding to the documentation.","title":"Document layout"},{"location":"contribute/documentation.html#creating-content","text":"MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page.","title":"Creating content"},{"location":"contribute/documentation.html#standard-markdown-features","text":"The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additiona; # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions.","title":"Standard Markdown features"},{"location":"contribute/documentation.html#links-within-mkdocs-generated-content","text":"MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognised as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file. When the site is generated the filename will be automatically converted to the correct URL","title":"Links within MkDocs generated content"},{"location":"contribute/documentation.html#extensions-used-in-the-prototype","text":"There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here","title":"Extensions used in the prototype"},{"location":"contribute/documentation.html#link-configuration","text":"Links on the page or embedded images can be annotated to control the links and also the appearance of the links:","title":"Link configuration"},{"location":"contribute/documentation.html#tabs","text":"Content can be organised into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World","title":"Tabs"},{"location":"contribute/documentation.html#information-boxes","text":"The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note","title":"Information boxes"},{"location":"contribute/documentation.html#setting-up-an-documentation-environment","text":"To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages `pip install -r requirements.txt' You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000. Everytime a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. Warning Within the configuration mkdocs is configured to fail with internal broken links. This will also kill the local test server. If you want to disable this functionality change the strict value in mkdocs.yml from strict: true to strict: false . Do not check in mkdocs.yaml with strict disabled, as we don't want to publish broken documentation!","title":"Setting up an documentation environment"},{"location":"contribute/governance.html","text":"Governance \u00b6 Roles and responsibilities \u00b6 As mentioned above, we value participation from anyone that is interested in this space. For the Cloud-Native Toolkit participation can take on a number of different forms. The following roles detail a number of ways in which people might interact with the Toolkit. Consumers \u00b6 Consumers are members of the community who are applying assets to their development projects. Anyone who wants to apply any of the assets can be a user. We encourage consumers to participate as evangelists and contributors as well. Evangelists \u00b6 Evangelists are members of the community who help others become consumers of the assets. They do so by: Advertising the assets and encouraging others to use them Supporting new consumers and answering questions, such as on Slack (IBM internal) Reporting bugs or missing features through GitHub issues Contributors \u00b6 Contributors are members of the community who help maintain, improve, and expand the assets. In addition to using and evangelizing the assets, they make the assets better by: Resolving issues in GitHub to fix bugs, add features, and improve documentation Submitting changes as GitHub pull requests Maintainers \u00b6 Project maintainers (aka maintainers) are owners of the project who are committed to the success of the assets in that project. Each project has a team of maintainers, and each team has a lead. In addition to their participation as contributors, maintainers have privileges to: Label, close, and manage GitHub issues Close and merge GitHub pull requests Nominate and vote on new maintainers Types of teams \u00b6 Core team \u00b6 Core team members are IBM employees responsible for the leadership and strategic direction of the set of Catalyst projects as a whole. The core team also directs how the Catalyst strategy will evolve with IBM Cloud product decisions. Core team responsibilities include: Actively engaging with the projects' communities Setting overall direction and vision Setting priorities and release schedule Focusing on broad, cross-cutting concerns Spinning up or shutting down project teams The core team will operate the technical steering committee. Technical steering committee \u00b6 The technical steering committee coordinates the project teams to ensure consistency between the projects and fosters collaboration between the core team and each project team. This close communication on cross-cutting concerns greatly mitigates the risk of misalignment that can come from decentralized efforts. The committee consists of the project leads of all of the projects as well as other members of the core team who may not presently be leading any projects. Project teams \u00b6 Each project team maintains the assets in its project. Therefore, its members are the maintainers of the assets. Each project operates independently, though it should follow this governance structure to define roles, responsibilities, and decision-making protocols. The project has a project lead, a lead maintainer who should also be a member of the technical steering committee. Each project lead is responsible for: - Acting as a point of primary contact for the team - Participating in the technical steering committee - Deciding on the initial membership of project maintainers (in consultation with the core team) - Determining and publishing project team policies and mechanics, including the way maintainers join and leave the team (which should be based on team consensus) - Communicating core vision to the team - Ensuring that issues and pull requests progress at a reasonable rate - Making final decisions in cases where the team is unable to reach consensus (should be rare) The way that project teams communicate internally and externally is left to each team, but: - Technical discussion should take place in the public domain as much as possible, ideally in GitHub issues and pull requests. - Each project should have a dedicated Slack channel (IBM internal). Decisions from Slack discussions should be captured in GitHub issues. - Project teams should actively seek out discussion and input from stakeholders who are not members of the team. Governance \u00b6 Planning \u00b6 Project planning is managed in a Kanban board , specifically this Zenhub board: Planning Zenhub (To Be Moved Externally) Decision-making \u00b6 Project teams should use consensus decision making as much as possible, but resort to lack of consensus decision making when necessary. Consensus \u00b6 Project teams use consensus decision-making with the premise that a successful outcome is not where one side of a debate has \"won,\" but rather where concerns from all sides have been addressed in some way. This emphatically does not mean design by committee, nor compromised design. Rather, it's a recognition that every design or implementation choice carries a trade-off and numerous costs. There is seldom a 100% right answer. Breakthrough thinking sometimes end up changing the playing field by eliminating tradeoffs altogether, but more often, difficult decisions have to be made. The key is to have a clear vision and set of values and priorities , which is the core team's responsibility to set and communicate, and the project teams' responsibility to act upon. Whenever possible, seek to reach consensus through discussion and design iteration. Concretely, the steps are: New GitHub issue or pull request is created with initial analysis of tradeoffs. Comments reveal additional drawbacks, problems, or tradeoffs. The issue or pull request is revised to address comments, often by improving the design or implementation. Repeat above until \"major objections\" are fully addressed, or it's clear that there is a fundamental choice to be made. Consensus is reached when most people are left with only \"minor\" objections. While they might choose the tradeoffs slightly differently, they do not feel a strong need to actively block the issue or pull request from progressing. One important question is: consensus among which people, exactly? Of course, the broader the consensus, the better. When a decision in a project team affects other teams (e.g. new/changed API), the team will be encouraged to invite people (e.g. leads) from affected teams. But at the very least, consensus within the members of the project team should be the norm for most decisions . If the core team has done its job of communicating the values and priorities, it should be possible to fit the debate about the issue into that framework and reach a fairly clear outcome. Lack of consensus \u00b6 In some cases, though, consensus cannot be reached. These cases tend to split into two very different camps: \"Trivial\" reasons , e.g., there is not widespread agreement about naming, but there is consensus about the substance. \"Deep\" reasons , e.g., the design fundamentally improves one set of concerns at the expense of another, and people on both sides feel strongly about it. In either case, an alternative form of decision-making is needed. For the \"trivial\" case, the project lead will make an executive decision or defer the decision to another maintainer on the team. For the \"deep\" case, the project lead is empowered to make a final decision, but should consult with the core team before doing so. Contribution process \u00b6 Catalyst assets are typically stored in GitHub repositories and use a fork and pull request workflow for contributions. Specific instructions can be found in each project's GitHub CONTRIBUTING.md file. Contributor License Agreement \u00b6 We require contributors outside of IBM to sign our Contributor License Agreement (CLA) before code contributions can be reviewed and merged. If you have questions, please contact the core team . Support \u00b6 Have questions? Found a bug? Learn where to go and what to do by visiting the Support page .","title":"Governance"},{"location":"contribute/governance.html#governance","text":"","title":"Governance"},{"location":"contribute/governance.html#roles-and-responsibilities","text":"As mentioned above, we value participation from anyone that is interested in this space. For the Cloud-Native Toolkit participation can take on a number of different forms. The following roles detail a number of ways in which people might interact with the Toolkit.","title":"Roles and responsibilities"},{"location":"contribute/governance.html#consumers","text":"Consumers are members of the community who are applying assets to their development projects. Anyone who wants to apply any of the assets can be a user. We encourage consumers to participate as evangelists and contributors as well.","title":"Consumers"},{"location":"contribute/governance.html#evangelists","text":"Evangelists are members of the community who help others become consumers of the assets. They do so by: Advertising the assets and encouraging others to use them Supporting new consumers and answering questions, such as on Slack (IBM internal) Reporting bugs or missing features through GitHub issues","title":"Evangelists"},{"location":"contribute/governance.html#contributors","text":"Contributors are members of the community who help maintain, improve, and expand the assets. In addition to using and evangelizing the assets, they make the assets better by: Resolving issues in GitHub to fix bugs, add features, and improve documentation Submitting changes as GitHub pull requests","title":"Contributors"},{"location":"contribute/governance.html#maintainers","text":"Project maintainers (aka maintainers) are owners of the project who are committed to the success of the assets in that project. Each project has a team of maintainers, and each team has a lead. In addition to their participation as contributors, maintainers have privileges to: Label, close, and manage GitHub issues Close and merge GitHub pull requests Nominate and vote on new maintainers","title":"Maintainers"},{"location":"contribute/governance.html#types-of-teams","text":"","title":"Types of teams"},{"location":"contribute/governance.html#core-team","text":"Core team members are IBM employees responsible for the leadership and strategic direction of the set of Catalyst projects as a whole. The core team also directs how the Catalyst strategy will evolve with IBM Cloud product decisions. Core team responsibilities include: Actively engaging with the projects' communities Setting overall direction and vision Setting priorities and release schedule Focusing on broad, cross-cutting concerns Spinning up or shutting down project teams The core team will operate the technical steering committee.","title":"Core team"},{"location":"contribute/governance.html#project-teams","text":"Each project team maintains the assets in its project. Therefore, its members are the maintainers of the assets. Each project operates independently, though it should follow this governance structure to define roles, responsibilities, and decision-making protocols. The project has a project lead, a lead maintainer who should also be a member of the technical steering committee. Each project lead is responsible for: - Acting as a point of primary contact for the team - Participating in the technical steering committee - Deciding on the initial membership of project maintainers (in consultation with the core team) - Determining and publishing project team policies and mechanics, including the way maintainers join and leave the team (which should be based on team consensus) - Communicating core vision to the team - Ensuring that issues and pull requests progress at a reasonable rate - Making final decisions in cases where the team is unable to reach consensus (should be rare) The way that project teams communicate internally and externally is left to each team, but: - Technical discussion should take place in the public domain as much as possible, ideally in GitHub issues and pull requests. - Each project should have a dedicated Slack channel (IBM internal). Decisions from Slack discussions should be captured in GitHub issues. - Project teams should actively seek out discussion and input from stakeholders who are not members of the team.","title":"Project teams"},{"location":"contribute/governance.html#governance_1","text":"","title":"Governance"},{"location":"contribute/governance.html#planning","text":"Project planning is managed in a Kanban board , specifically this Zenhub board: Planning Zenhub (To Be Moved Externally)","title":"Planning"},{"location":"contribute/governance.html#decision-making","text":"Project teams should use consensus decision making as much as possible, but resort to lack of consensus decision making when necessary.","title":"Decision-making"},{"location":"contribute/governance.html#contribution-process","text":"Catalyst assets are typically stored in GitHub repositories and use a fork and pull request workflow for contributions. Specific instructions can be found in each project's GitHub CONTRIBUTING.md file.","title":"Contribution process"},{"location":"contribute/governance.html#contributor-license-agreement","text":"We require contributors outside of IBM to sign our Contributor License Agreement (CLA) before code contributions can be reviewed and merged. If you have questions, please contact the core team .","title":"Contributor License Agreement"},{"location":"contribute/governance.html#support","text":"Have questions? Found a bug? Learn where to go and what to do by visiting the Support page .","title":"Support"},{"location":"learning/analysis.html","text":"","title":"Static Analysis"},{"location":"learning/dev-setup.html","text":"Cloud-Native Toolkit Developer Setup \u00b6 Todo This will be the setup needed for a developer to run through the learning content - Prerequisites - Install required CLI tools - Git setup (Personal Access Token) Todo Should we offer different experiences? - Local laptop - LMS system (no local tooling/setup required) - Hosted Shell in web browser (no local tooling/setup required) Todo Should we create the ArgoCD GitOps repo as part of developer setup - this way the gitops stage in the pipeline will run automatically - saving an additional run after configuration in the Continuous Delivery stage? Is creating the CD gitops repo considered a developer setup activity - or should it be within the CD section?","title":"Developer setup"},{"location":"learning/dev-setup.html#cloud-native-toolkit-developer-setup","text":"Todo This will be the setup needed for a developer to run through the learning content - Prerequisites - Install required CLI tools - Git setup (Personal Access Token) Todo Should we offer different experiences? - Local laptop - LMS system (no local tooling/setup required) - Hosted Shell in web browser (no local tooling/setup required) Todo Should we create the ArgoCD GitOps repo as part of developer setup - this way the gitops stage in the pipeline will run automatically - saving an additional run after configuration in the Continuous Delivery stage? Is creating the CD gitops repo considered a developer setup activity - or should it be within the CD section?","title":"Cloud-Native Toolkit Developer Setup"},{"location":"learning/fast-cd.html","text":"Continuous Delivery - fast start \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo. Configuring GitOps with Argo CD \u00b6 Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution Set up the GitOps repo \u00b6 Todo Is this better in the developer setup section? Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD Starter Kit used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD Starter Kit . If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test Hook the CI pipeline to the CD pipeline \u00b6 The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the respositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc. If you used the IGC CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the IGC CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the IGC CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD. Configure Release namespaces \u00b6 ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Creat a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets Register the GitOps repo in ArgoCD \u00b6 Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops Create a project in Argo CD \u00b6 In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note: The --dest and --src arguments can be provided multiple times if there are multiple destations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f project.yaml -n tools Add an application in Argo CD for each application component \u00b6 Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The ArgoCD project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) revision - The Git branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluter where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools Managing secrets in Argo CD \u00b6 The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation . Prepare the Key Protect instance \u00b6 As the name suggests, the Argo CD Key Protect plugin leverages the capabilities of the Key Protect service to manage the protected information. The details for setting up and managing the Key Protect instance can be found in Secret management with Key Protect . From those instructions you can find the information required for the subsequent steps. Create the secret configuration \u00b6 The input to the plugin is a directory that contains one or more yaml \"secret templates\". In this case the \"secret template\" provides the structure of the desired secret with placeholders for the values that will be pulled from the key management system. Create a directory to contain the secret configuration. The Argo CD Starter Kit repository has a template in templates/secrets-plugin that can be copied as a starting point Update the values in the yaml file for the secret that will be created apiVersion : keymanagement.ibm/v1 kind : SecretTemplate metadata : name : mysecret annotations : key-manager : key-protect key-protect/instanceId : instance-id key-protect/region : us-east spec : labels : {} annotations : {} values : - name : url value : https://ibm.com - name : username b64value : dGVhbS1jYXA= - name : password keyId : 36397b07-d98d-4c0b-bd7a-d6c290163684 The metadata.annotations value is optional. key-manager - the only value supported currently is key-protect key-protect/instanceId - the instance id of the key protect instance. If not provided then the instance-id value from the key-protect-access secret will be used. key-protect/region - the region where the key protect instance has been provisioned. If not provided then the region value from the key-protect-access secret will be used. The metadata.name value given will be used as the name for the Secret that will be generated. The information in spec.labels and spec.annotations will be copied over as the labels and annotations in the Secret that is generated The spec.values section contains the information that should be provided in the data section of the generated Secret. There are three prossible ways the values can be provided: value - the actual value can be provided directly as clear text. This would be appropriate for information that is not sensitive but is required in the secret b64value - a base64 encoded value can be provided to the secret. This can be used for large values that might present formatting issues or for information that is not sensitive but that might be obfuscated a bit (like a username) keyId - the id (not the name) of the Standard Key that has been stored in Key Protect. The value stored in Key Protect can be anything Commit the changes to the GitOps repository Add the secret application in Argo CD \u00b6 Once the configuration has been added to the GitOps repository, Argo CD needs to be configured to deploy the secrets. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored revision - The branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed Plugin - In the last section of the UI select Plugin from the dropdown key-protect-secret - Click in the name field and select key-protect-secret from the dropdown Repeat that step for each secret application and environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On this is the same as the UI host --sso is an optional flag used when SSO authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } \\ --config-management-plugin key-protect-secret where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named secret-application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } plugin : name : key-protect-secret syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluter where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f secret-application.yaml -n tools Configure another cluster as an Argo CD deployment target \u00b6 Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the cluster api server can be selected as a deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"Continuous Delivery"},{"location":"learning/fast-cd.html#continuous-delivery-fast-start","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo.","title":"Continuous Delivery - fast start"},{"location":"learning/fast-cd.html#configuring-gitops-with-argo-cd","text":"Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution","title":"Configuring GitOps with Argo CD"},{"location":"learning/fast-cd.html#set-up-the-gitops-repo","text":"Todo Is this better in the developer setup section? Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD Starter Kit used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD Starter Kit . If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test","title":"Set up the GitOps repo"},{"location":"learning/fast-cd.html#hook-the-ci-pipeline-to-the-cd-pipeline","text":"The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the respositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc. If you used the IGC CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the IGC CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the IGC CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD.","title":"Hook the CI pipeline to the CD pipeline"},{"location":"learning/fast-cd.html#configure-release-namespaces","text":"ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Creat a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets","title":"Configure Release namespaces"},{"location":"learning/fast-cd.html#register-the-gitops-repo-in-argocd","text":"Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops","title":"Register the GitOps repo in ArgoCD"},{"location":"learning/fast-cd.html#create-a-project-in-argo-cd","text":"In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note: The --dest and --src arguments can be provided multiple times if there are multiple destations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f project.yaml -n tools","title":"Create a project in Argo CD"},{"location":"learning/fast-cd.html#add-an-application-in-argo-cd-for-each-application-component","text":"Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The ArgoCD project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) revision - The Git branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluter where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools","title":"Add an application in Argo CD for each application component"},{"location":"learning/fast-cd.html#managing-secrets-in-argo-cd","text":"The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation .","title":"Managing secrets in Argo CD"},{"location":"learning/fast-cd.html#configure-another-cluster-as-an-argo-cd-deployment-target","text":"Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the cluster api server can be selected as a deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"Configure another cluster as an Argo CD deployment target"},{"location":"learning/fast-ci.html","text":"Continuous Integration - fast start \u00b6 Overview \u00b6 The environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. The approach for getting started is exactly the same for an environment based on Kubernetes or Red Hat OpenShift . Note The instructions provided below lean heavily on the use of the IGC Command Line Interface (CLI) tool, to both show how the CLI works in context and to streamline the process (the reason for creating the CLI in the first place). However, the use of the CLI is in no way required to use the Cloud-Native Toolkit. If you would prefer to work through these instructions without the use of the CLI, we have provided the equivalent manual steps for each command on the Cloud-Native Toolkit CLI page. The video below (click to play the video) demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster. Create an application \u00b6 1. Log into your Development Cluster from the command line \u00b6 Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Todo Why $VARIABLES used below? Now Windows friendly! Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: Todo Are we using oc for both Kubernetes and Kubernetes installation - does the developer setup enable this? oc console Todo The link for CRC below is not available - what should this be? Note If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding 2. Create the development namespace \u00b6 Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ DEV_NAMESPACE } 3. Open the Developer Dashboard \u00b6 The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of preconfigured Starter Kits that make seeding your development project very easy. Before starting, open a browser and make sure you are logged into Github . There are two options for how to access the dashboard: OpenShift console Open the Application Launcher dropdown from the top-right and select Developer Dashboard Command Line oc dashboard 4. Create your app in Git \u00b6 Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Warning Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. From the Developer Dashboard, click on Starter Kits tab Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Starter Kit Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization. Create the DevOps pipeline \u00b6 5. Register the application in a DevOps Pipeline \u00b6 Info We will be using the pipeline command of the IBM Garage Cloud cli to register the DevOps pipeline. The pipeline command gives an option for both Jenkins and Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline. oc pipeline ${ GIT_URL } For example: oc pipeline https://github.com/gct-showcase/inventory-svc For the deployment of your first app with OpenShift select Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console. 6. View your application pipeline \u00b6 The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. Tekton OpenShift 4.x Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Kubernetes Open the Developer Dashboard kubectl dashboard Select the Tekton tile to launch the Tekton UI Select your development project Jenkins OpenShift 4.x Open the OpenShift Web Console oc console OR From the left-hand menu, select Builds -> Build Configs Select your project from the drop-down menu at the top The registered pipeline should appear in the list Kubernetes Run the command oc dashboard in your terminal to open your Developer Dashboard Select the Jenkins tool to open the Jenkins dashboard Run the command kubectl credentials in your terminal to get the list of logins for the tools Use the Jenkins userid and password to log into the Jenkins dashboard Wait for the pipeline stages to start building. Once the stages have completed, you will see a view similar to the one below. 7. View your application artifacts \u00b6 Todo What happens if the cluster is not running on IBM Cloud - registry? The pipeline built two artifacts for deploying your app: Container image -- The image registry includes a container image with your app built in Helm chart -- The artifact repository includes a Helm chart repository that includes a Helm chart for deploying your app Let's look at these artifacts in the Toolkit environment. The container image is stored in the IBM Cloud Container Registry: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Image Registry. In the image regisry, you'll see the image the pipeline built for your app, such as us.icr.io/isv-scenarios/stockbffnode-bw with a different tag for each build. The Helm chart is stored in Artifactory: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Artifactory. In the Artifactory console, select Artifactory > Artifacts > generic-local. You'll see a isv-scenarios folder with a different chart for each build, such as generic-local/isv-scenarios/stockbffnode-bw-0.0.1.tgz . 8. Access the running app \u00b6 Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note Be sure the namespace context is set correctly before running the following commands Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected. 9. Locate the app in the web console \u00b6 The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry. After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. OpenShift 4.x Open the OpenShift web console oc console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Kubernetes Open the Kubernetes Dashboard kubectl console Change to the namespace from default to either dev or the namespace you used to deploy your app Click on Deployments You should see the deployment of your application Click on your application , and the corresponding Replica Set Try scaling the application, click on Scale in the header, change number of pods to 2 and click OK Click on one of the pod instances Click on Logs You can see the running state of your application Navigate around the console to understand where your deployment, service and pods are running Success You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it. Run the application locally \u00b6 10. Clone your code to you local machine \u00b6 Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Clone the repository using the url from the terminal. git clone ${ GIT_URL } For example: git clone https://github.com/gct-showcase/inventory-svc You will be required to enter your GitHub User ID and use your Git Hub Personal Access Token as your password. This will complete the clone of your git repository. Change into the cloned directory cd stockbffnode 11. Run the application locally \u00b6 Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:3000/api-docs/ You can try out the sample API that is provided with this Code Pattern You can now add new features and function from inside the Cloud Shell and experiment with your code before you push any changes back to git. 12. Test the webhook \u00b6 Go to your cloned git project and navigate to chart/base directory. cd stockbffnode cd chart/base Open the file Chart.yaml in edit mode and change the description field's value from \"A Helm chart for Kubernetes\" to \"A Helm chart for [yourprojectName]\" Save the edits Push the changes back to your repository git add . git commit -m \"Update application name\" git push As soon as you push your code changes successfully, the webhook will trigger a new pipeline run for your project in your namespace in OCP. Note if the webhook registration step failed, the git push will not trigger the pipeline.","title":"Continuous Integration"},{"location":"learning/fast-ci.html#continuous-integration-fast-start","text":"","title":"Continuous Integration - fast start"},{"location":"learning/fast-ci.html#overview","text":"The environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. The approach for getting started is exactly the same for an environment based on Kubernetes or Red Hat OpenShift . Note The instructions provided below lean heavily on the use of the IGC Command Line Interface (CLI) tool, to both show how the CLI works in context and to streamline the process (the reason for creating the CLI in the first place). However, the use of the CLI is in no way required to use the Cloud-Native Toolkit. If you would prefer to work through these instructions without the use of the CLI, we have provided the equivalent manual steps for each command on the Cloud-Native Toolkit CLI page. The video below (click to play the video) demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster.","title":"Overview"},{"location":"learning/fast-ci.html#create-an-application","text":"","title":"Create an application"},{"location":"learning/fast-ci.html#1-log-into-your-development-cluster-from-the-command-line","text":"Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Todo Why $VARIABLES used below? Now Windows friendly! Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: Todo Are we using oc for both Kubernetes and Kubernetes installation - does the developer setup enable this? oc console Todo The link for CRC below is not available - what should this be? Note If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding","title":"1. Log into your Development Cluster from the command line"},{"location":"learning/fast-ci.html#2-create-the-development-namespace","text":"Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ DEV_NAMESPACE }","title":"2. Create the development namespace"},{"location":"learning/fast-ci.html#3-open-the-developer-dashboard","text":"The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of preconfigured Starter Kits that make seeding your development project very easy. Before starting, open a browser and make sure you are logged into Github . There are two options for how to access the dashboard: OpenShift console Open the Application Launcher dropdown from the top-right and select Developer Dashboard Command Line oc dashboard","title":"3. Open the Developer Dashboard"},{"location":"learning/fast-ci.html#4-create-your-app-in-git","text":"Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Warning Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. From the Developer Dashboard, click on Starter Kits tab Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Starter Kit Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization.","title":"4. Create your app in Git"},{"location":"learning/fast-ci.html#create-the-devops-pipeline","text":"","title":"Create the DevOps pipeline"},{"location":"learning/fast-ci.html#5-register-the-application-in-a-devops-pipeline","text":"Info We will be using the pipeline command of the IBM Garage Cloud cli to register the DevOps pipeline. The pipeline command gives an option for both Jenkins and Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline. oc pipeline ${ GIT_URL } For example: oc pipeline https://github.com/gct-showcase/inventory-svc For the deployment of your first app with OpenShift select Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console.","title":"5. Register the application in a DevOps Pipeline"},{"location":"learning/fast-ci.html#6-view-your-application-pipeline","text":"The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. Tekton OpenShift 4.x Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Kubernetes Open the Developer Dashboard kubectl dashboard Select the Tekton tile to launch the Tekton UI Select your development project Jenkins OpenShift 4.x Open the OpenShift Web Console oc console OR From the left-hand menu, select Builds -> Build Configs Select your project from the drop-down menu at the top The registered pipeline should appear in the list Kubernetes Run the command oc dashboard in your terminal to open your Developer Dashboard Select the Jenkins tool to open the Jenkins dashboard Run the command kubectl credentials in your terminal to get the list of logins for the tools Use the Jenkins userid and password to log into the Jenkins dashboard Wait for the pipeline stages to start building. Once the stages have completed, you will see a view similar to the one below.","title":"6. View your application pipeline"},{"location":"learning/fast-ci.html#7-view-your-application-artifacts","text":"Todo What happens if the cluster is not running on IBM Cloud - registry? The pipeline built two artifacts for deploying your app: Container image -- The image registry includes a container image with your app built in Helm chart -- The artifact repository includes a Helm chart repository that includes a Helm chart for deploying your app Let's look at these artifacts in the Toolkit environment. The container image is stored in the IBM Cloud Container Registry: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Image Registry. In the image regisry, you'll see the image the pipeline built for your app, such as us.icr.io/isv-scenarios/stockbffnode-bw with a different tag for each build. The Helm chart is stored in Artifactory: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Artifactory. In the Artifactory console, select Artifactory > Artifacts > generic-local. You'll see a isv-scenarios folder with a different chart for each build, such as generic-local/isv-scenarios/stockbffnode-bw-0.0.1.tgz .","title":"7. View your application artifacts"},{"location":"learning/fast-ci.html#8-access-the-running-app","text":"Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note Be sure the namespace context is set correctly before running the following commands Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected.","title":"8. Access the running app"},{"location":"learning/fast-ci.html#9-locate-the-app-in-the-web-console","text":"The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry. After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. OpenShift 4.x Open the OpenShift web console oc console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Kubernetes Open the Kubernetes Dashboard kubectl console Change to the namespace from default to either dev or the namespace you used to deploy your app Click on Deployments You should see the deployment of your application Click on your application , and the corresponding Replica Set Try scaling the application, click on Scale in the header, change number of pods to 2 and click OK Click on one of the pod instances Click on Logs You can see the running state of your application Navigate around the console to understand where your deployment, service and pods are running Success You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it.","title":"9. Locate the app in the web console"},{"location":"learning/fast-ci.html#run-the-application-locally","text":"","title":"Run the application locally"},{"location":"learning/fast-ci.html#10-clone-your-code-to-you-local-machine","text":"Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Clone the repository using the url from the terminal. git clone ${ GIT_URL } For example: git clone https://github.com/gct-showcase/inventory-svc You will be required to enter your GitHub User ID and use your Git Hub Personal Access Token as your password. This will complete the clone of your git repository. Change into the cloned directory cd stockbffnode","title":"10. Clone your code to you local machine"},{"location":"learning/fast-ci.html#11-run-the-application-locally","text":"Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:3000/api-docs/ You can try out the sample API that is provided with this Code Pattern You can now add new features and function from inside the Cloud Shell and experiment with your code before you push any changes back to git.","title":"11. Run the application locally"},{"location":"learning/fast-ci.html#12-test-the-webhook","text":"Go to your cloned git project and navigate to chart/base directory. cd stockbffnode cd chart/base Open the file Chart.yaml in edit mode and change the description field's value from \"A Helm chart for Kubernetes\" to \"A Helm chart for [yourprojectName]\" Save the edits Push the changes back to your repository git add . git commit -m \"Update application name\" git push As soon as you push your code changes successfully, the webhook will trigger a new pipeline run for your project in your namespace in OCP. Note if the webhook registration step failed, the git push will not trigger the pipeline.","title":"12. Test the webhook"},{"location":"learning/fast-start.html","text":"Learning to use the Cloud-Native Toolkit \u00b6 This section provides a fast-start learning path to using the Cloud-Native Toolkit. The content is divided into 3 sections: Setup - ensure you have the required tools, accounts and environment needed to use the toolkit Continuous Integration - Learn how to build, test and deliver your code ready for deployment Continuous Delivery - Learn how to use gitops and automation to deploy your applications to a cloud environment","title":"Learning Overview"},{"location":"learning/fast-start.html#learning-to-use-the-cloud-native-toolkit","text":"This section provides a fast-start learning path to using the Cloud-Native Toolkit. The content is divided into 3 sections: Setup - ensure you have the required tools, accounts and environment needed to use the toolkit Continuous Integration - Learn how to build, test and deliver your code ready for deployment Continuous Delivery - Learn how to use gitops and automation to deploy your applications to a cloud environment","title":"Learning to use the Cloud-Native Toolkit"},{"location":"learning/in-depth.html","text":"Cloud-Native Development with the Toolkit \u00b6 Todo Add the following content: Quick summary of Cloud-Native development Outline of what the Continuous Integration phase needs to complete Role of starter kits in toolkit Todo Ensure that links to reference section are included whenever a specific tool, script or starter kit is mentioned Testing \u00b6 Todo Add the following content: Language specific, automated unit testing Code coverage tools Static Analysis \u00b6 Todo Add the following content: What the analysis tools achieve what can be covered by static analysis (code, Dockerfile) Security and vunerability scanning Pipeline \u00b6 Todo Add the following content: Pipeline technology comparison (Jenkins vs Tekton) What the pipeline needs to deliver - everything needed by Continuous Delivery to deliver into testing and production Build the application (Language specific - install dependencies, compile/link code, ...) Run automated unit tests Perform analysis Package into container Perform security scans Build Helm package","title":"Cloud-Native Development with the Toolkit"},{"location":"learning/in-depth.html#cloud-native-development-with-the-toolkit","text":"Todo Add the following content: Quick summary of Cloud-Native development Outline of what the Continuous Integration phase needs to complete Role of starter kits in toolkit Todo Ensure that links to reference section are included whenever a specific tool, script or starter kit is mentioned","title":"Cloud-Native Development with the Toolkit"},{"location":"learning/in-depth.html#testing","text":"Todo Add the following content: Language specific, automated unit testing Code coverage tools","title":"Testing"},{"location":"learning/in-depth.html#static-analysis","text":"Todo Add the following content: What the analysis tools achieve what can be covered by static analysis (code, Dockerfile) Security and vunerability scanning","title":"Static Analysis"},{"location":"learning/in-depth.html#pipeline","text":"Todo Add the following content: Pipeline technology comparison (Jenkins vs Tekton) What the pipeline needs to deliver - everything needed by Continuous Delivery to deliver into testing and production Build the application (Language specific - install dependencies, compile/link code, ...) Run automated unit tests Perform analysis Package into container Perform security scans Build Helm package","title":"Pipeline"},{"location":"learning/pipeline.html","text":"","title":"Pipeline"},{"location":"learning/testing.html","text":"","title":"Automated Testing"},{"location":"overview/overview.html","text":"What is the Cloud-Native Toolkit? \u00b6 Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift and Kubernetes. It embodies IBM Garage Method principles and practices for consistently developed applications, incorporating best practices that increase developer velocity for efficient delivery of business value. Cloud-Native Toolkit objectives \u00b6 There are a number of objectives behind providing the Cloud-Native Toolkit. The three main goals of the Toolkit are provided below: 1. Accelerate time to business value \u00b6 One goal of the Cloud-Native Toolkit is to prepare the development environment quickly and allow the development team to start delivering business function on day one of the first iteration using enterprise cloud-native practices. After all, the point of cloud-native development is to deliver business value to end users and the development and operations infrastructure are provided in service to that goal. Through the automation provided by the Cloud-Native Toolkit we can provision an environment in minutes through automation that is fully configured and ready for a development team to start working immediately. With the other components of the Toolkit, developers can begin with a rich DevOps framework with a focus on \"build to manage\" techniques to help build production-ready applications. 2. Reduce risk through consistent delivery models from start to production \u00b6 The Cloud-Native Toolkit encapsulates many of the available best practices for cloud-native development including DevOps and \"Build to Manage\" practices. They have been provided through the Toolkit in this way so that developers and SREs can benefit from these practices without requiring any additional effort and so that they can be applied consistently from project to project. 3. Quickly ramp up development teams on Red Hat OpenShift and Kubernetes \u00b6 Containerized platforms like Red Hat OpenShift and Kubernetes provide a great deal of functionality and flexibility for application teams. However, these platforms can at time seem unapproachable for developers and SREs new to the environment given all the different concepts and components. The Cloud-Native Toolkit aims to help with the learning curve in two different ways: Provide tools and techniques to \"round off the corners\" of the more complex aspects of working in a containerized environment Provide a learning journey that incrementally introduces the concepts of a containerized environment in terms of practical scenarios, not abstract theory Guided walk-though \u00b6 If you'd like to have a guided walkthrough of what the Cloud-Native Toolkit provides, check out this video demonstration of the Toolkit in action. Components of the Cloud-Native Toolkit \u00b6 As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life-cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Guides - this set of documentation that weaves the various toolkit components together with a perspective on how to apply cloud-native practices to deliver business solutions Infrastructure as Code - Terraform scripts and GitOps configuration to provision and manage the environment CLI - a simple node-based CLI that installs as a plugin to the kubectl and oc CLIs and provides commands to simplify common Developer Dashboard - Dashboard component and Red Hat OpenShift console extensions to simplify common developer activities DevOps pipelines - continuous integration pipelines for Tekton and Jenkins Starter Kits and Code Patterns - software repositories that can be used to quickly get started building applications using common patterns, or to serve as a reference to enhance existing patterns Learning Journey - activation material to teach practitioners how to apply cloud-native practices in real-world scenarios using the Toolkit Cloud-Native Toolkit Developer Environment \u00b6 The Cloud-Native Toolkit Developer Environment includes several features that support IBM Garage Method best practices for consistent and rapid development of cloud-native applications: Cluster : A Red Hat OpenShift or Kubernetes cluster that both hosts the tools and itself is a deployment target for application builds SDLC : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools Starter Kits : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : A centralized console to help developers use the environment's capabilities Typically a Cloud System Admin (or a squad lead ) installs and sets up a new Developer Environment after the inception workshop , providing a place for the developers to start developing the minimum viable product (MVP) . The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform scripts structured as modular components so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development. Red Hat Open Innovation Labs CI/CD components embodies a very similar approach to how they deliver success with OpenShift. Environment components \u00b6 After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift or IBM Cloud Kubernetes Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. Logo Usage Reference Logo Usage Reference Artifactory is an Open Source product maintained by JFrog Jenkins Open Source project Jenkins SonarQube Open Source project maintained by SonarSource Nexus Repository Open Source project maintained by SonaType Trivy Open Source project maintained by Aqua InteliJ IDE from JetBrains VSCode Free IDE maintained by Microsoft Jaeger Open Source tool maintained by Jaeger Community ArgoCD Open Source tool maintained by ArgoCD Community OpenShift and CodeReady Workspaces are products from Red Hat LogDNA IBM Cloud service supplied by LogDNA Sysdig IBM Cloud service supplied by Sysdig The tools to provision an environment using the Cloud-Native Toolkit can the customized to provision a particular set of tools fit for the environment. The Toolkit provides a default installation to provision a Developer Environment as a starting point. Any of the available components listed on the Terraform modules page can be used to prepare the environment. Development cluster \u00b6 The heart of the Developer Environment is a cluster: An IBM Cloud-managed Kubernetes or Red Hat OpenShift cluster Cluster namespace that encapsulates the tooling installed in the cluster: tools A collection of SRE tools and services Continuous delivery tools \u00b6 The following best-of-breed open source software tools are installed in the cluster's tools namespace: Capability Tool Bitnami Description Continuous Integration Jenkins CI Yes Jenkins is a common tool for Continuous Integration Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube Yes SonarQube can scan code and display the results in a dashboard Container Image Registry IBM Cloud Container Registry Stores container images to be deployed Artifact Management Artifactory Yes Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser Backend services \u00b6 The following IBM Cloud services can be created and bound to the cluster: Capability Service Description Log Management Log Analysis with LogDNA Manage and analyze app logs Monitoring Cloud Monitoring with Sysdig Monitor app and platform resources User Authentication App ID Verify identities of clients accessing app end points Relational Data Storage Databases For PostgreSQL Stores relational data structured as schemas for SQL querying Schemaless Data Storage Cloudant NoSQL database for JSON documents Binary Data Storage Cloud Object Storage Storage service commonly used for binary content","title":"What is the Cloud Native Toolkit"},{"location":"overview/overview.html#what-is-the-cloud-native-toolkit","text":"Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift and Kubernetes. It embodies IBM Garage Method principles and practices for consistently developed applications, incorporating best practices that increase developer velocity for efficient delivery of business value.","title":"What is the Cloud-Native Toolkit?"},{"location":"overview/overview.html#cloud-native-toolkit-objectives","text":"There are a number of objectives behind providing the Cloud-Native Toolkit. The three main goals of the Toolkit are provided below:","title":"Cloud-Native Toolkit objectives"},{"location":"overview/overview.html#1-accelerate-time-to-business-value","text":"One goal of the Cloud-Native Toolkit is to prepare the development environment quickly and allow the development team to start delivering business function on day one of the first iteration using enterprise cloud-native practices. After all, the point of cloud-native development is to deliver business value to end users and the development and operations infrastructure are provided in service to that goal. Through the automation provided by the Cloud-Native Toolkit we can provision an environment in minutes through automation that is fully configured and ready for a development team to start working immediately. With the other components of the Toolkit, developers can begin with a rich DevOps framework with a focus on \"build to manage\" techniques to help build production-ready applications.","title":"1. Accelerate time to business value"},{"location":"overview/overview.html#2-reduce-risk-through-consistent-delivery-models-from-start-to-production","text":"The Cloud-Native Toolkit encapsulates many of the available best practices for cloud-native development including DevOps and \"Build to Manage\" practices. They have been provided through the Toolkit in this way so that developers and SREs can benefit from these practices without requiring any additional effort and so that they can be applied consistently from project to project.","title":"2. Reduce risk through consistent delivery models from start to production"},{"location":"overview/overview.html#3-quickly-ramp-up-development-teams-on-red-hat-openshift-and-kubernetes","text":"Containerized platforms like Red Hat OpenShift and Kubernetes provide a great deal of functionality and flexibility for application teams. However, these platforms can at time seem unapproachable for developers and SREs new to the environment given all the different concepts and components. The Cloud-Native Toolkit aims to help with the learning curve in two different ways: Provide tools and techniques to \"round off the corners\" of the more complex aspects of working in a containerized environment Provide a learning journey that incrementally introduces the concepts of a containerized environment in terms of practical scenarios, not abstract theory","title":"3. Quickly ramp up development teams on Red Hat OpenShift and Kubernetes"},{"location":"overview/overview.html#guided-walk-though","text":"If you'd like to have a guided walkthrough of what the Cloud-Native Toolkit provides, check out this video demonstration of the Toolkit in action.","title":"Guided walk-though"},{"location":"overview/overview.html#components-of-the-cloud-native-toolkit","text":"As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life-cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Guides - this set of documentation that weaves the various toolkit components together with a perspective on how to apply cloud-native practices to deliver business solutions Infrastructure as Code - Terraform scripts and GitOps configuration to provision and manage the environment CLI - a simple node-based CLI that installs as a plugin to the kubectl and oc CLIs and provides commands to simplify common Developer Dashboard - Dashboard component and Red Hat OpenShift console extensions to simplify common developer activities DevOps pipelines - continuous integration pipelines for Tekton and Jenkins Starter Kits and Code Patterns - software repositories that can be used to quickly get started building applications using common patterns, or to serve as a reference to enhance existing patterns Learning Journey - activation material to teach practitioners how to apply cloud-native practices in real-world scenarios using the Toolkit","title":"Components of the Cloud-Native Toolkit"},{"location":"overview/overview.html#cloud-native-toolkit-developer-environment","text":"The Cloud-Native Toolkit Developer Environment includes several features that support IBM Garage Method best practices for consistent and rapid development of cloud-native applications: Cluster : A Red Hat OpenShift or Kubernetes cluster that both hosts the tools and itself is a deployment target for application builds SDLC : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools Starter Kits : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : A centralized console to help developers use the environment's capabilities Typically a Cloud System Admin (or a squad lead ) installs and sets up a new Developer Environment after the inception workshop , providing a place for the developers to start developing the minimum viable product (MVP) . The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform scripts structured as modular components so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development. Red Hat Open Innovation Labs CI/CD components embodies a very similar approach to how they deliver success with OpenShift.","title":"Cloud-Native Toolkit Developer Environment"},{"location":"overview/overview.html#environment-components","text":"After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift or IBM Cloud Kubernetes Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. Logo Usage Reference Logo Usage Reference Artifactory is an Open Source product maintained by JFrog Jenkins Open Source project Jenkins SonarQube Open Source project maintained by SonarSource Nexus Repository Open Source project maintained by SonaType Trivy Open Source project maintained by Aqua InteliJ IDE from JetBrains VSCode Free IDE maintained by Microsoft Jaeger Open Source tool maintained by Jaeger Community ArgoCD Open Source tool maintained by ArgoCD Community OpenShift and CodeReady Workspaces are products from Red Hat LogDNA IBM Cloud service supplied by LogDNA Sysdig IBM Cloud service supplied by Sysdig The tools to provision an environment using the Cloud-Native Toolkit can the customized to provision a particular set of tools fit for the environment. The Toolkit provides a default installation to provision a Developer Environment as a starting point. Any of the available components listed on the Terraform modules page can be used to prepare the environment.","title":"Environment components"},{"location":"overview/prerequisites.html","text":"Assumed Knowledge \u00b6 The Cloud-Native Toolkit has been developed to help developers adopt best practices when developing enterprise cloud native applications and services. It aims to teach developers how to adopt Continuous Integration and Continuous Delivery best practices, but assumes a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Prerequisets"},{"location":"overview/prerequisites.html#assumed-knowledge","text":"The Cloud-Native Toolkit has been developed to help developers adopt best practices when developing enterprise cloud native applications and services. It aims to teach developers how to adopt Continuous Integration and Continuous Delivery best practices, but assumes a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Assumed Knowledge"},{"location":"overview/whats-new.html","text":"What's new \u00b6 Jan 28, 2021 \u00b6 Cloud Native Toolkit Workshop released. The workshop in a box environment is easy and quick to setup with hands on labs including videos. Check them out at cloudnativetoolkit.dev/workshop . More hands on labs for the workshop coming soon. Jan 6, 2021 \u00b6 CLI \u00b6 v1.11.1 , v1.11.0 , v1.10.2 , v1.10.1 , v1.10.0 , v1.9.0 , v1.8.1 , v1.8.0 , v1.7.1 , v1.7.0 , v1.6.0 , v1.5.0 Many usability changes, particularly for the pipeline command: Reduces required permissions At the start of every command that needs access to the kube api, the cli checks that a connection is available. Previously it did that by trying to list all the pods in the cluster (e.g. the equivalent of kubectl get pods -A ). Unfortunately, that command needs a great deal of access to succeed. The check was changed to run a command that requires much less permission. Before creating the webhook triggers, the pipeline command would read the Tekton version number from annotations on the operator deployment in the openshift-operators namespace. This check required a great deal of permissions to be able to read the deployment in that namespace. Instead, the pipeline command has been changed to resort to a brute force check - it assumes v0.6.0 and if it fails tries again with v0.4.0. Usability updates for pipeline command Allows the repo url to be passed in so it is not necessary to clone the repository first Creates a single event listener per namespace/project instead of a new event listener for each repo Detects the runtime of the repository and filters the tekton pipelines based on the runtime Reads params from tekton pipeline and prompts for values The input arguments have been cleaned up to remove conflicts and to use values that make more sense for the input parameters. Dec 11, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.5.0 Updates ibm-container-platform module to v1.18.3 to provision ocp 4.5 clusters properly Adds option of storage class for Artifactory Updates to point releases of terraform modules with updated workflows to generate module catalogs argocd v2.10.1 artifactory v1.10.0 dashboard v1.10.4 ibm-image-registry v1.2.3 ocp-image-registry v1.2.2 k8s-image-registry v1.1.5 k8s-source-control v1.2.1 jenkins v1.4.3 pactbroker v1.4.2 sonarqube v1.9.2 swaggereditor v1.4.1 tekton v2.0.2 tekton-resources v2.2.0 ibm-logdna v2.4.3 ibm-sysdig v2.3.3 CLI \u00b6 v1.4.2 , v1.4.1 Prints next steps to the console after calling the pipeline command Pipeline run started: memcached-operator-catalog-1762ff0a6d7 Next steps: Tekton cli: View PipelineRun info - tkn pr describe memcached-operator-catalog-1762ff0a6d7 View PipelineRun logs - tkn pr logs memcached-operator-catalog-1762ff0a6d7 OpenShift console: View PipelineRun - https://console-openshift-console.garage-dev-ocp45-vpc-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud/k8s/ns/operator-dev/tekton.dev~v1beta1~PipelineRun/memcached-operator-catalog-1762ff0a6d7 Registers the gitops command as a plugin to kubectl and oc clis Tekton tasks \u00b6 v2.2.3 , v2.2.2 , v2.2.1 , v2.2.0 , v2.1.27 Updates tasks to use images hosted in quay.io instead of docker.io to avoid rate limiting issue Adds workflow to mirror required images from docker.io to quay.io on a nightly schedule Adds pipelines for operator and operator catalog builds Nov 20, 2020 \u00b6 CLI \u00b6 v1.4.0 , v1.3.0 , v1.2.2 , v1.2.1 Refactors Git server interaction logic to make more extensible Adds support to pipeline command for Gogs git server running in cluster Adds support to pipeline command for Bitbucket along with existing support for GitHub, GitHub Enterprise, and GitLab Nov 13, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.4.0 Adds image-registry and source-control modules Updates numbering for generated tiles Updates default settings when installing from iZero Updates underlying module versions ibm-container-platform v1.18.0 artifactory v1.9.2 dashboard v1.10.0 ibm-image-registry v1.2.0 ocp-image-registry v1.2.0 tools-tekton-resources v2.1.9 k8s-source-control v1.2.0 tools-swagger-editor v1.4.0 Tekton tasks \u00b6 v2.1.26 , v2.1.25 , v2.1.24 , v2.1.23 , v2.1.24 , v2.1.21 , v2.1.20 Updates tekton tasks to support Gogs git server running in cluster Fixes setup task to handle different characters in git url Updates the task order in the pipelines to release the helm chart after the scan Defaults to using the internal OCP registry if none is defined Nov 6, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.3.9 , v2.3.8 , v2.3.7 , v2.3.6 , v2.3.5 , v2.3.4 , v2.3.3 , v2.3.2 , v2.3.1 , v2.3.0 , v2.2.2 Prints the elapsed time for the Toolkit installation process Updates tile definition to include README.md in long description and update input parameters Updates module versions ibm-container-platform v1.18.0 ibm-object-storage v2.0.1 CLI \u00b6 v1.2.0 , v1.1.0 Simplifies the logic used to determine cluster type by using intrinsic information within the cluster. This expands the number of commands that can be run against a cluster that doesn't have the toolkit installed Updates the git secret logic to support older versions of the git cli (which allows the CLI to be run in the IBM OpenLab environment) Oct 30, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.2.1 , v2.2.0 Adds quick install option with Terraform job running within the cluster Updates Tekton Resources module to v2.1.8 Update terraform modules to latest dashboard v1.9.0 ocp-cluster v2.3.5 pactbroker v1.4.0 CLI \u00b6 v1.0.3 , v1.0.2 Fixes bug that causes the endpoint command to fail due to a missing import Fixes bug with the credentials command that caused the internal urls to be displayed instead of the external ones Tekton tasks \u00b6 v2.1.19 , v2.1.18 , v2.1.17 , v2.1.16 , v2.1.15 , v2.1.14 , v2.1.13 , v2.1.12 , v2.1.11 , v2.1.10 , v2.1.9 , v2.1.8 , v2.1.7 , v2.1.6 , v2.1.5 , v2.1.4 , v2.1.3 Fixes bug in deploy task when the Git hash has an \"e\" in it (tries to convert to an exponential number) Combines Trivy and IBM VA scan into one task Uses internal endpoints for tools hosted within the cluster (like artifactory and sonarqube) Updates from helm v2 to v3 for the pipeline logic Fix the health url check logic Uses registry-access to get image registry information instead of ibmcloud-config Adds logic to wait for Vulnerability Advisor to complete before testing the result Sept 25, 2020 \u00b6 Tekton tasks \u00b6 v2.1.2 , v2.1.1 , v2.1.0 , Introduces image vulnerability scan with Aquasec Trivy Fixes trivy scan logic to check for PERFORM_SCAN flag in setup and execute steps Sept 11, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.1.0 , v2.0.2 , v2.0.1 Introduced Key Protect ArgoCD plugin in argocd module to v2.9.0 to generate kubernetes secrets from key material in Key Protect Updates namespace module to v2.6.0 to remove use of previously deprecated, now removed --export flag Aug 25, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.0.0 Updates Tekton module and resources to support the Red Hat Tekton operator and related versions Simplifies the process to install the Cloud-Native Toolkit on a Red Hat OpenShift provisioned anywhere Provide Private Catalog tile for install with Schematics Improves the handling of LogDNA and Sysdig in the cluster Automates the post-install configuration steps for Artifactory Automates the post-install configuration steps for SonarQube CLI \u00b6 v1.0.1 Updates tekton pipeline handling to create the webhook Adds git , gitops and console commands Tekton tasks \u00b6 v2.0.3 Refactors tasks and pipelines to support v1beta1 schema and remove dependency on PipelineResources Streamlines CI process in pipelines to be more modular and reusable Tasks for Vulnerability scanning with IBM Image Registry","title":"What's New"},{"location":"overview/whats-new.html#whats-new","text":"","title":"What's new"},{"location":"overview/whats-new.html#jan-28-2021","text":"Cloud Native Toolkit Workshop released. The workshop in a box environment is easy and quick to setup with hands on labs including videos. Check them out at cloudnativetoolkit.dev/workshop . More hands on labs for the workshop coming soon.","title":"Jan 28, 2021"},{"location":"overview/whats-new.html#jan-6-2021","text":"","title":"Jan 6, 2021"},{"location":"overview/whats-new.html#dec-11-2020","text":"","title":"Dec 11, 2020"},{"location":"overview/whats-new.html#nov-20-2020","text":"","title":"Nov 20, 2020"},{"location":"overview/whats-new.html#nov-13-2020","text":"","title":"Nov 13, 2020"},{"location":"overview/whats-new.html#nov-6-2020","text":"","title":"Nov 6, 2020"},{"location":"overview/whats-new.html#oct-30-2020","text":"","title":"Oct 30, 2020"},{"location":"overview/whats-new.html#sept-25-2020","text":"","title":"Sept 25, 2020"},{"location":"overview/whats-new.html#sept-11-2020","text":"","title":"Sept 11, 2020"},{"location":"overview/whats-new.html#aug-25-2020","text":"","title":"Aug 25, 2020"},{"location":"reference/cli.html","text":"Cloud Native Toolkit - Command Line Interface \u00b6 Invoking the CLI \u00b6 When the CLI is installed, it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: $ igc --help IBM Garage Cloud Native Toolkit CLI (https://cloudnativetoolkit.dev) Usage: igc <command> [args] Commands: igc console Launch the IKS or OpenShift admin console igc create-webhook Create a git webhook for a given Jenkins pipeline igc credentials Lists the urls and credentials for the tools deployed to the cluster igc dashboard Open the Developer Dashboard in the default browser igc enable Enable the current repository with pipeline logic igc endpoints List the current ingress hosts for deployed apps in a namespace [aliases: ingress, endpoint, ingresses] igc git-secret [name] Create a kubernetes secret that contains the url, username, and personal access token for a git repo igc git [remote] Launches a browser to the git repo url specified by the remote. If not provided remote defaults to origin igc gitops Registers the git repository in the kubernetes cluster as the gitops repository for the given namespace igc sync [namespace] Create a namespace (if it does not exist) and prepare it with the necessary configuration [aliases: project, namespace] igc pull-secret [namespace] Copy pull secrets into the provided project from the template namespace igc pipeline [gitUrl] Register a pipeline for the current code repository igc tool-config [name] Create the config map and secret for a tool configured in the environment igc vlan Print out the vlan values igc yq <command> lightweight yaml command-line processor that addresses deficiencies with the existing `yq` command Options: --version Show version number [boolean] --help Show help [boolean] Info As of v0.5.1, the IGC CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all of the following are equivalent: igc pipeline kubectl pipeline oc pipeline Prerequisite tools \u00b6 Some of the commands provided by the IGC CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools , in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud) Available commands \u00b6 dashboard \u00b6 Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace where the dashboard has been deployed; the default is tools Usage Todo Add Windows Powershell equivalents CLI The command is used in the following way: igc dashboard OpenShift The following commands would have the same result on OpenShift: HOST = $( oc get routes/dashboard -n tools -o jsonpath = '{.spec.host}' ) open \"https:// $HOST \" Kubernetes The following commands would have the same result on Kubernetes: HOST = $( kubectl get ingress/developer-dashboard -n tools -o jsonpath = '{.spec.rules[0].host}' ) open \"https:// $HOST \" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command console \u00b6 Opens the IKS or OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established. Usage CLI The command is used in the following way: igc console OpenShift The following commands would have the same result on OpenShift: open $( oc whoami --show-console ) Kubernetes The following commands would have the same result on Kubernetes: REGION = \"...\" CLUSTER_NAME = \"...\" CLUSTER_ID = $( ibmcloud ks cluster get --cluster ${ CLUSTER_NAME } | grep -E \"^ID\" | sed -E \"s/ID: +([^ ]+)/\\\\1/g\" ) open \"https:// ${ REGION } .containers.cloud.ibm.com/kubeproxy/clusters/ ${ CLUSTER_ID } /service/#/overview?namespace=default\" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command git \u00b6 Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out. Usage CLI The command is used in the following way: igc git If you have multiple remotes and would like to open one other than origin : igc git origin-fork Manual The following commands would have the same result with shell commands: alias gh = \"open https://github. $( git config remote.origin.url | cut -f2 -d. | tr ':' / ) \" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command credentials \u00b6 Lists the endpoints, user names, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace where the tools have been deployed; the default is tools Usage CLI The command is used in the following way: igc credentials The credential output is JSON format like this Credentials: { argocd: { user: 'admin' , password: '12345678' , url: 'https://argocd-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . dashboard: { url: 'https://dashboard-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . } OpenShift or Kubernetes The following commands have the same result (note the dependency on jq ): # config maps kubectl get configmap -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-config\").data]' # secrets kubectl get secret -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-apikey\").data | with_entries(.value |= @base64d)]' Related commands dashboard : displays the url of the Developer Dashboard and launches the default browser tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command endpoints \u00b6 Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace from which the endpoints will be read; the value will be read from the current context if not provided Usage CLI The command is used in the following way: igc endpoints OpenShift The following commands list the route and ingress endpoints: # routes kubectl get route -n tools # ingress kubectl get ingress -n tools Kubernetes The following commands list the ingress endpoints: kubectl get ingress -n tools sync \u00b6 Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --verbose flag to produce more verbose logging [boolean] Usage CLI Create a dev namespace for development igc sync myapp-dev OpenShift Create a dev namespace for development oc sync myapp-dev Kubernetes Create a dev namespace for development kubectl sync myapp-dev Manual ConfigMap and Secret setup The following steps will copy the ConfigMaps and Secrets from a template namespace to a target namespace: export TEMPLATE_NAMESPACE = \"tools\" export NAMESPACE = \"NAMESPACE\" kubectl get configmap -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get configmap ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done kubectl get secret -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get secret ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done pull-secret \u00b6 Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean] Usage CLI Copy the pull secret from default namespace into myapp-test namepsace and add to serviceAccount default igc pull-secret myapp-test -t default -z default Manual pull secret setup The following commands will copy the pull secret(s) from the default namespace and add them to the service account: export NAMESPACE = \"myapp-test\" export SERVICE_ACCOUNT = \"default\" if [[ $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | grep icr | wc -l | xargs ) -eq 0 ]] ; then echo \"*** Copying pull secrets from default namespace to ${ NAMESPACE } namespace\" kubectl get secrets -n default | grep icr | sed \"s/\\([A-Za-z-]*\\) *.*/\\1/g\" | while read default_secret ; do kubectl get secret ${ default_secret } -n default -o yaml --export | sed \"s/name: default-/name: /g\" | kubectl -n ${ NAMESPACE } create -f - done else echo \"*** Pull secrets already exist on ${ NAMESPACE } namespace\" fi EXISTING_SECRETS = $( kubectl get serviceaccount/ ${ SERVICE_ACCOUNT } -n \" ${ NAMESPACE } \" -o json | tr '\\n' ' ' | sed -E \"s/.*imagePullSecrets.: \\[([^]]*)\\].*/\\1/g\" | grep icr | wc -l | xargs ) if [[ ${ EXISTING_SECRETS } -eq 0 ]] ; then echo \"*** Adding secrets to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" PULL_SECRETS = $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ \"{\\\"name\\\": \\\"\"}{ .metadata.name }{ \"\\\"}\\n\" }{ end }' | grep icr | grep -v \" ${ NAMESPACE } \" | paste -sd \",\" - ) kubectl patch -n \" ${ NAMESPACE } \" serviceaccount/ ${ SERVICE_ACCOUNT } -p \"{\\\"imagePullSecrets\\\": [ ${ PULL_SECRETS } ]}\" else echo \"*** Pull secrets already applied to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" fi pipeline \u00b6 Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url. Repository location \u00b6 The pipeline command supports registering a CI pipeline for a repository that has been cloned locally or using the remote repository url. Local repository \u00b6 If you are registering a local repository then you must run the command from within the directory of your local clone of the Git repo. When registering a local repository, the pipeline will use the branch that is currently checked out. Remote repository \u00b6 To register a remote repository, pass the repo url as an argument to the pipeline command. For example: oc pipeline \"https://github.com/my-org/my-repo\" You can optionally provide the branch name with the url using a hash ( # ): oc pipeline \"https://github.com/my-org/my-repo#my-branch\" Note: When registering a remote git repo, if the branch is not provided then the default branch will be used. Pipeline type \u00b6 The pipeline command supports registering pipelines with either Tekton or Jenkins. The pipeline can be specified from the command-line with either the --tekton or --jenkins flags. If a flag is not provided then you will be prompted to select the pipeline. Git credentials \u00b6 The command will prompt for the username and password/personal access token to access the Git repository, unless those are already stored in a secret in the cluster namespace or provided as command-line parameters. The username and password can be provided with the -u and -p flags. If you want to change the credentials that have already been stored in the cluster namespace, the -g argument an be provided and you will be prompted for the credentials. Tekton template pipeline \u00b6 If a Tekton pipeline will be used, a template pipeline must be selected for the new repository pipeline. The command reads the template pipelines available in the template namespace. The template namespace can be provided with the -t argument and will default to tools if not provided. The command will also filter the list of pipelines based on the runtime determined from the given repository. If there is more than one template pipeline available then you will be prompted to pick one. The template pipeline can also be provided on the command-line using the --pipeline argument. If the name doesn't match an available pipeline then you will be prompted to select one. Pipeline parameters \u00b6 Once the pipeline template is selected, you will be prompted to provide values for the defined pipeline parameters. The values can also be provided from the command-line using the -p argument. The name of the parameter is listed at the beginning of the prompt message. Multiple parameters can be provided by repeating the -p argument. For example: oc pipeline --tekton \"https://github.com/my-org/my-repo\" -p scan-image = false -p edge = false Optional arguments \u00b6 -u : the username for accessing the Git repo -P : the password or personal access token for accessing the Git repo -g : ignore existing git-credentials secret and prompt to update the values -p : provide parameters for the pipeline --jenkins : deploy using a Jenkins pipeline --tekton : deploy using a Tekton pipeline --pipeline : the name of the Tekton pipeline -n : the deployment namespace; if not provided the namespace from the current context will be used -t : the template namespace; if not provided the value will default to tools Usage CLI Create a Jenkins pipeline in the current namespace and prompt for the Git credentials oc pipeline --jenkins Create a Tekton pipeline in the my-dev namespace, using the Git credentials gituser and gitpat oc pipeline -n my-dev -u gituser -P gitpat --tekton Manual Steps for Tekton The following is the list of steps required to manually configure a Tekton pipeline with your development cluster. Set the current namespace/project OpenShift ```shell oc project {namespace} ``` Kubernetes ```shell kubectl config set-context --current --namespace={namespace} ``` Copy the tasks from the tools namespace into the current namespace kubectl get tasks -o json -n tools | \\ jq 'del(.items[].metadata.uid) | del(.items[].metadata.selfLink) | del(.items[].metadata.resourceVersion) | del(.items[].metadata.namespace) | del(.items[].metadata.creationTimestamp) | del(.items[].metadata.generation) | del(.items[].metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - List the available pipeline templates in the tools namespace and select the one to use for your project. kubectl get pipelines -n tools Clone the selected pipeline from the tools namespace into the current namespace kubectl get pipeline ${ TEMPLATE_NAME } -o json -n tools | \\ jq --arg PIPELINE_NAME ${ PIPELINE_NAME } '.metadata.name = $PIPELINE_NAME | del(.metadata.uid) | del(.metadata.selfLink) | del(.metadata.resourceVersion) | del(.metadata.namespace) | del(.metadata.creationTimestamp) | del(.metadata.generation) | del(.metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - where: - TEMPLATE_NAME is the name of the pipeline selected in the previous step - PIPELINE_NAME is the name of the pipeline for your project Start the pipeline \u00b6 The Tekton pipeline does not automatically start when it is first created. After the webhook is created in the subsequent steps the pipeline will start when changes are pushed to the repository but before that, we can manually trigger the build to start using the CLI. (The pipeline can also be started through the OpenShift Console.) Kick off the pipeline using the Tekton CLI tkn pipeline start { PIPELINE_NAME } -s pipeline -p git-url ={ GIT_REPO } -p git-revision ={ GIT_BRANCH } To create a new PipelineRun with the same parameters from a previous PipelineRun you can do the following tkn pipeline start { PIPELINE_NAME } --use-pipelinerun { PIPELINE_RUN_NAME } Create a Git Webhook** \u00b6 Create the event listener and triggers \u00b6 In order for a Tekton pipeline to be triggered by a webhook notification, several resources need to be created: TriggerTemplate - defines how to create the PipelineRun and any other required resources when a webhook notification is received. TriggerBinding - provides a mapping for the information available in the webhook payload into the TriggerTemplate EventListener - makes the connection between the Pipeline, TriggerBinding, and TriggerTemplate together that will be created when a webhook is triggered Create a file named tekton-trigger.yaml and paste in the following contents: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_TEMPLATE_NAME } spec : params : - description : The git revision name : gitrevision - description : The git repository url name : gitrepositoryurl resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : generateName : { PIPELINE_NAME } - spec : params : - name : git-url value : $(params.gitrepositoryurl) - name : git-revision value : $(params.gitrevision) - name : scan-image value : \"false\" pipelineRef : name : { PIPELINE_NAME } --- apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerBinding metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_BINDING_NAME } spec : params : - name : gitrevision value : $(body.head_commit.id) - name : gitrepositoryurl value : $(body.repository.url) --- apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : { PIPELINE_NAME } name : { EVENT_LISTENER_NAME } spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding name : { TRIGGER_BINDING_NAME } interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/{BRANCH_NAME}' name : { PIPELINE_NAME } template : name : { TRIGGER_TEMPLATE_NAME } Replace the place holder values with the appropriate values: where: - {PIPELINE_NAME} is the name of your Pipeline resource from the previous section. - {TRIGGER_TEMPLATE_NAME} is the name of the TriggerTemplate. This can be the same as the {PIPELINE_NAME} . - {TRIGGER_BINDING_NAME} is the name of the TriggerBinding. This can be the same as the {PIPELINE_NAME} . - {EVENT_LISTENER_NAME} is the name of the EventListener. This can be el-{PIPELINE_NAME} if the EventListeners will be configured one-to-one with the Pipelines or the instance can be shared across the project. - {BRANCH_NAME} is the name of the branch from which webhook events should trigger the build to start Apply the trigger resources to the cluster, in the same namespace where the Pipeline was created kubectl apply -f tekton-trigger.yaml In order for the Git repository to trigger the build with a webhook, an endpoint needs to be available. Expose the EventListener service with a route to provide that endpoint. oc expose service ${ EVENT_LISTENER_NAME } --name = ${ EVENT_LISTENER_NAME } Register the webhook url with your Git repository \u00b6 The particular steps will vary to create the Webhook depending on the flavor of hosted Git you are using (GitHub, GitHub Enterprise, GitLab, BitBucket, etc) but the general flow will remain the same. Get the host name for the route created in the previous step oc get route ${ EVENT_LISTENER_NAME } -o jsonpath = '{.spec.host}' Create a webhook in your hosted Git repository using the https url of the host name from the previous step that is triggered by the desired events (e.g. push, pull request, release) Manual steps for Jenkins on OpenShift Provision Jenkins ephemeral \u00b6 Jenkins ephemeral provides a kubernetes native version of Jenkins that dynamically provisions build agents on-demand. It's ephemeral meaning it doesn't allocate any persistent storage in the cluster. Set the project/namespace oc project { NAMESPACE } where: - {NAMESPACE} is the development namespace where the pipelines will run Run the following command to provision the Jenkins instance in your namespace oc new-app jenkins-ephemeral Open the OpenShift console as described in the login steps above Select Workloads -> Pods from the left-hand menu At the top of the page select your project/namespace from the drop-down list to see the Jenkins instance running Give the jenkins service account privileged access \u00b6 All of the Cloud-Native Toolkit pipelines use buildah to build and push the container image to the registry. Unfortunately, the buildah container must run as root. By default, OpenShift does not allow containers to run as the root user and special permission is required for the pipeline to run. With the Jenkins build engine, all the build processes run as the jenkins service account. In order for the pipeline container to run as root on OpenShift we will need to give the privileged security context constraint (scc) to jenkins service account with the following command: oc project { NAMESPACE } oc adm policy add-scc-to-user privileged -z jenkins where: - {NAMESPACE} should be the name you claimed in the box note prefixed to -dev (e.g. user01-dev) Create a secret with git credentials \u00b6 In order for Jenkins to have access to the git repository, particularly if it is a private repository, a Kubernetes secret needs to be added that contains the git credentials. Create a personal access token (if you don't already have one) using the prereq instructions - https://cloudnativetoolkit.dev/getting-started/prereqs#configure-github-personal-access-token Copy the following into a file called gitsecret.yaml and update the {Git-Username}, and {Git-PAT} apiVersion : v1 kind : Secret metadata : annotations : build.openshift.io/source-secret-match-uri-1 : https://github.com/* labels : jenkins.io/credentials-type : usernamePassword name : git-credentials type : kubernetes.io/basic-auth stringData : username : { Git-Username } password : { Git-PAT } where: - Git-Username is the username that has access to the git repo - Git-PAT is the personal access token of the git user After logging into the cluster, create the secret by running the following: oc project { NAMESPACE } oc create -f gitsecret.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run Create the build config \u00b6 On OpenShift 4.3, Jenkins is built into the OpenShift console and the build pipelines can be managed using Kubernetes custom resources. The following steps will create one by hand to create the build pipeline for the new application. Copy the following into a file called buildconfig.yaml and update the {Name}, {Secret}, {Git-Repo-URL}, and {Namespace} apiVersion : v1 kind : BuildConfig metadata : name : { Name } spec : triggers : - type : GitHub github : secret : my-secret-value source : git : uri : { Git-Repo-URL } ref : master strategy : jenkinsPipelineStrategy : jenkinsfilePath : Jenkinsfile env : - name : CLOUD_NAME value : openshift - name : NAMESPACE value : { NAMESPACE } where: - Name is in the name of your pipeline - Git-Repo-URL is the https url to the git repository - {NAMESPACE} is the development namespace where the pipelines will run Assuming you are still logged into the cluster, create the buildconfig resource in the cluster oc project { NAMESPACE } oc create -f buildconfig.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run View the pipeline in the OpenShift console \u00b6 Open the OpenShift console for the cluster Select Builds -> Build Config Select your project/namespace (i.e. {NAMESPACE} ) from the top The build pipeline that was created in the previous step should appear Manually trigger the pipeline by selecting Start Build the menu button on the right side of the row Create the webhook \u00b6 Run the following to get the webhook details from the build config oc project { NAMESPACE } oc describe bc { Name } where: - {Name} is the name used in the previous step for the build config - {NAMESPACE} is the development namespace where the pipelines will run The webhook url will have a structure similar to: http://{openshift_api_host:port}/oapi/v1/namespaces/{namespace}/buildconfigs/{name}/webhooks/{secret}/generic In this case {secret} will be my-secret-value Open a browser to the GitHub repo deployed in the previous step in the build config Select Settings then Webhooks . Press Add webhook Paste the webhook url from the previous step into the Payload url Set the content-type to application/json and leave the rest of the values as the defaults Press Add webhook to create the webhook Press the button to test the webhook to ensure that everything was done properly Go back to your project code and push a change to one of the files Go to the Build pipeline page in the OpenShift console to see that the build was triggered Manual steps for Jenkins on Kubernetes TBD enable \u00b6 Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Jenkinsfile This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run igc pipeline to connect your repo to a pipeline in the environment. Command flags - --repo : the set of pipelines to choose from; the default is https://github.com/ibm-garage-cloud/garage-pipelines - -p : the name of the pipeline that should be installed; if not provided then you will be prompted - -b : the branch from which the pipeline should be installed; the default is stable - r : the version number of the pipeline that should be installed; the default is latest Usage CLI Before running the command, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Apply the pipeline updates using the CLI command igc enable Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them Manual steps The follow provides the manual steps equivalent to the igc enable command: Before updating the pipelines, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Download the index.yaml file containing the available pipeline versions curl -O https://ibm-garage-cloud.github.io/garage-pipelines/index.yaml Look through the index.yaml file to identify the url for the desired pipeline branch and version With the PIPELINE_URL from the previous step, run the following to download the pipeline tar-ball curl -O ${ PIPELINE_URL } Extract the tar-ball into your repository directory. You will be prompted to overwrite files. Overwrite as appropriate tar xzf ${ PIPELINE_FILE } Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them git-secret \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed. Command flags [positional] : overwrites the name of the config map -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage ===\"CLI\" The following gives an example of using the git-secret command to set up the config map and secret in the dev namespace ```shell igc git-secret -n dev ``` Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap showcase.myrepo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master gitops \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed. Command flags -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage CLI The following gives an example of using the gitops command to set up the config map and secret in the dev namespace igc gitops -n dev Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap github-repo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master tool-config \u00b6 Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard. Command flags The name for the tool -n : the tools namespace; the default is tools --url : the endpoint for accessing the tool, usually its dashboard --username : (optional) the user name for logging into to tool --password : (optional) the password for logging into to tool Usage CLI The following gives an example of using the tool-config command to set up a tool named my-tool with its dashboard's endpoint and credentials igc tool-config my-tool \\ --url https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --username admin \\ --password password Manual install with helm The following gives an example of using helm directly to do the equivalent (using helm 3): helm install my-tool tool-config \\ --repo https://ibm-garage-cloud.github.io/toolkit-charts/ \\ --set url = https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --set username = admin \\ --set password = password vlan \u00b6 Lists the VLANs for a particular IBM Cloud region. This information is useful for preparing Terraform cluster creation steps. The command reads all the data centers in the region and allows you to select the appropriate data center for the vlan. This command requires that the terminal is already logged in to the cloud region. It does NOT need to be logged in to a cluster. Usage CLI List a pair of public/private VLANs for a new environment to use igc vlan Manual steps List the zones for the region ibmcloud ks zones --region-only --provider classic Select the desired zone from the listing provided by the previous command and run the following to list the vlans for that zone ibmcloud ks vlans --zone ${ zone }","title":"Command Line Tool"},{"location":"reference/cli.html#cloud-native-toolkit-command-line-interface","text":"","title":"Cloud Native Toolkit - Command Line Interface"},{"location":"reference/cli.html#invoking-the-cli","text":"When the CLI is installed, it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: $ igc --help IBM Garage Cloud Native Toolkit CLI (https://cloudnativetoolkit.dev) Usage: igc <command> [args] Commands: igc console Launch the IKS or OpenShift admin console igc create-webhook Create a git webhook for a given Jenkins pipeline igc credentials Lists the urls and credentials for the tools deployed to the cluster igc dashboard Open the Developer Dashboard in the default browser igc enable Enable the current repository with pipeline logic igc endpoints List the current ingress hosts for deployed apps in a namespace [aliases: ingress, endpoint, ingresses] igc git-secret [name] Create a kubernetes secret that contains the url, username, and personal access token for a git repo igc git [remote] Launches a browser to the git repo url specified by the remote. If not provided remote defaults to origin igc gitops Registers the git repository in the kubernetes cluster as the gitops repository for the given namespace igc sync [namespace] Create a namespace (if it does not exist) and prepare it with the necessary configuration [aliases: project, namespace] igc pull-secret [namespace] Copy pull secrets into the provided project from the template namespace igc pipeline [gitUrl] Register a pipeline for the current code repository igc tool-config [name] Create the config map and secret for a tool configured in the environment igc vlan Print out the vlan values igc yq <command> lightweight yaml command-line processor that addresses deficiencies with the existing `yq` command Options: --version Show version number [boolean] --help Show help [boolean] Info As of v0.5.1, the IGC CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all of the following are equivalent: igc pipeline kubectl pipeline oc pipeline","title":"Invoking the CLI"},{"location":"reference/cli.html#prerequisite-tools","text":"Some of the commands provided by the IGC CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools , in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud)","title":"Prerequisite tools"},{"location":"reference/cli.html#available-commands","text":"","title":"Available commands"},{"location":"reference/cli.html#dashboard","text":"Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace where the dashboard has been deployed; the default is tools Usage Todo Add Windows Powershell equivalents CLI The command is used in the following way: igc dashboard OpenShift The following commands would have the same result on OpenShift: HOST = $( oc get routes/dashboard -n tools -o jsonpath = '{.spec.host}' ) open \"https:// $HOST \" Kubernetes The following commands would have the same result on Kubernetes: HOST = $( kubectl get ingress/developer-dashboard -n tools -o jsonpath = '{.spec.rules[0].host}' ) open \"https:// $HOST \" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command","title":"dashboard"},{"location":"reference/cli.html#console","text":"Opens the IKS or OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established. Usage CLI The command is used in the following way: igc console OpenShift The following commands would have the same result on OpenShift: open $( oc whoami --show-console ) Kubernetes The following commands would have the same result on Kubernetes: REGION = \"...\" CLUSTER_NAME = \"...\" CLUSTER_ID = $( ibmcloud ks cluster get --cluster ${ CLUSTER_NAME } | grep -E \"^ID\" | sed -E \"s/ID: +([^ ]+)/\\\\1/g\" ) open \"https:// ${ REGION } .containers.cloud.ibm.com/kubeproxy/clusters/ ${ CLUSTER_ID } /service/#/overview?namespace=default\" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command","title":"console"},{"location":"reference/cli.html#git","text":"Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out. Usage CLI The command is used in the following way: igc git If you have multiple remotes and would like to open one other than origin : igc git origin-fork Manual The following commands would have the same result with shell commands: alias gh = \"open https://github. $( git config remote.origin.url | cut -f2 -d. | tr ':' / ) \" Related commands credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command","title":"git"},{"location":"reference/cli.html#credentials","text":"Lists the endpoints, user names, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace where the tools have been deployed; the default is tools Usage CLI The command is used in the following way: igc credentials The credential output is JSON format like this Credentials: { argocd: { user: 'admin' , password: '12345678' , url: 'https://argocd-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . dashboard: { url: 'https://dashboard-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . } OpenShift or Kubernetes The following commands have the same result (note the dependency on jq ): # config maps kubectl get configmap -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-config\").data]' # secrets kubectl get secret -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-apikey\").data | with_entries(.value |= @base64d)]' Related commands dashboard : displays the url of the Developer Dashboard and launches the default browser tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command","title":"credentials"},{"location":"reference/cli.html#endpoints","text":"Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established. Command flags - -n : the namespace from which the endpoints will be read; the value will be read from the current context if not provided Usage CLI The command is used in the following way: igc endpoints OpenShift The following commands list the route and ingress endpoints: # routes kubectl get route -n tools # ingress kubectl get ingress -n tools Kubernetes The following commands list the ingress endpoints: kubectl get ingress -n tools","title":"endpoints"},{"location":"reference/cli.html#sync","text":"Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --verbose flag to produce more verbose logging [boolean] Usage CLI Create a dev namespace for development igc sync myapp-dev OpenShift Create a dev namespace for development oc sync myapp-dev Kubernetes Create a dev namespace for development kubectl sync myapp-dev Manual ConfigMap and Secret setup The following steps will copy the ConfigMaps and Secrets from a template namespace to a target namespace: export TEMPLATE_NAMESPACE = \"tools\" export NAMESPACE = \"NAMESPACE\" kubectl get configmap -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get configmap ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done kubectl get secret -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get secret ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done","title":"sync"},{"location":"reference/cli.html#pull-secret","text":"Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean] Usage CLI Copy the pull secret from default namespace into myapp-test namepsace and add to serviceAccount default igc pull-secret myapp-test -t default -z default Manual pull secret setup The following commands will copy the pull secret(s) from the default namespace and add them to the service account: export NAMESPACE = \"myapp-test\" export SERVICE_ACCOUNT = \"default\" if [[ $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | grep icr | wc -l | xargs ) -eq 0 ]] ; then echo \"*** Copying pull secrets from default namespace to ${ NAMESPACE } namespace\" kubectl get secrets -n default | grep icr | sed \"s/\\([A-Za-z-]*\\) *.*/\\1/g\" | while read default_secret ; do kubectl get secret ${ default_secret } -n default -o yaml --export | sed \"s/name: default-/name: /g\" | kubectl -n ${ NAMESPACE } create -f - done else echo \"*** Pull secrets already exist on ${ NAMESPACE } namespace\" fi EXISTING_SECRETS = $( kubectl get serviceaccount/ ${ SERVICE_ACCOUNT } -n \" ${ NAMESPACE } \" -o json | tr '\\n' ' ' | sed -E \"s/.*imagePullSecrets.: \\[([^]]*)\\].*/\\1/g\" | grep icr | wc -l | xargs ) if [[ ${ EXISTING_SECRETS } -eq 0 ]] ; then echo \"*** Adding secrets to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" PULL_SECRETS = $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ \"{\\\"name\\\": \\\"\"}{ .metadata.name }{ \"\\\"}\\n\" }{ end }' | grep icr | grep -v \" ${ NAMESPACE } \" | paste -sd \",\" - ) kubectl patch -n \" ${ NAMESPACE } \" serviceaccount/ ${ SERVICE_ACCOUNT } -p \"{\\\"imagePullSecrets\\\": [ ${ PULL_SECRETS } ]}\" else echo \"*** Pull secrets already applied to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" fi","title":"pull-secret"},{"location":"reference/cli.html#pipeline","text":"Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url.","title":"pipeline"},{"location":"reference/cli.html#enable","text":"Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Jenkinsfile This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run igc pipeline to connect your repo to a pipeline in the environment. Command flags - --repo : the set of pipelines to choose from; the default is https://github.com/ibm-garage-cloud/garage-pipelines - -p : the name of the pipeline that should be installed; if not provided then you will be prompted - -b : the branch from which the pipeline should be installed; the default is stable - r : the version number of the pipeline that should be installed; the default is latest Usage CLI Before running the command, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Apply the pipeline updates using the CLI command igc enable Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them Manual steps The follow provides the manual steps equivalent to the igc enable command: Before updating the pipelines, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Download the index.yaml file containing the available pipeline versions curl -O https://ibm-garage-cloud.github.io/garage-pipelines/index.yaml Look through the index.yaml file to identify the url for the desired pipeline branch and version With the PIPELINE_URL from the previous step, run the following to download the pipeline tar-ball curl -O ${ PIPELINE_URL } Extract the tar-ball into your repository directory. You will be prompted to overwrite files. Overwrite as appropriate tar xzf ${ PIPELINE_FILE } Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them","title":"enable"},{"location":"reference/cli.html#git-secret","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed. Command flags [positional] : overwrites the name of the config map -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage ===\"CLI\" The following gives an example of using the git-secret command to set up the config map and secret in the dev namespace ```shell igc git-secret -n dev ``` Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap showcase.myrepo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master","title":"git-secret"},{"location":"reference/cli.html#gitops","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed. Command flags -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage CLI The following gives an example of using the gitops command to set up the config map and secret in the dev namespace igc gitops -n dev Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap github-repo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master","title":"gitops"},{"location":"reference/cli.html#tool-config","text":"Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard. Command flags The name for the tool -n : the tools namespace; the default is tools --url : the endpoint for accessing the tool, usually its dashboard --username : (optional) the user name for logging into to tool --password : (optional) the password for logging into to tool Usage CLI The following gives an example of using the tool-config command to set up a tool named my-tool with its dashboard's endpoint and credentials igc tool-config my-tool \\ --url https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --username admin \\ --password password Manual install with helm The following gives an example of using helm directly to do the equivalent (using helm 3): helm install my-tool tool-config \\ --repo https://ibm-garage-cloud.github.io/toolkit-charts/ \\ --set url = https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --set username = admin \\ --set password = password","title":"tool-config"},{"location":"reference/cli.html#vlan","text":"Lists the VLANs for a particular IBM Cloud region. This information is useful for preparing Terraform cluster creation steps. The command reads all the data centers in the region and allows you to select the appropriate data center for the vlan. This command requires that the terminal is already logged in to the cloud region. It does NOT need to be logged in to a cluster. Usage CLI List a pair of public/private VLANs for a new environment to use igc vlan Manual steps List the zones for the region ibmcloud ks zones --region-only --provider classic Select the desired zone from the listing provided by the previous command and run the following to list the vlans for that zone ibmcloud ks vlans --zone ${ zone }","title":"vlan"},{"location":"reference/git.html","text":"Cloud Native Toolkit Git Repositories \u00b6 The Cloud Native Toolkit is developed in the open, with all content being managed in github repositories. This page provides a guide to the key repositories. The primary organization is https://github.com/ibm-garage-cloud","title":"Git repositories"},{"location":"reference/git.html#cloud-native-toolkit-git-repositories","text":"The Cloud Native Toolkit is developed in the open, with all content being managed in github repositories. This page provides a guide to the key repositories. The primary organization is https://github.com/ibm-garage-cloud","title":"Cloud Native Toolkit Git Repositories"},{"location":"reference/reference.html","text":"","title":"Reference"},{"location":"reference/starter-kit/base.html","text":"","title":"Base template"},{"location":"reference/starter-kit/new-starterkit.html","text":"","title":"Create a Starter Kit"},{"location":"reference/starter-kit/starter-kit.html","text":"Starter Kits \u00b6","title":"Starter Kit overiew"},{"location":"reference/starter-kit/starter-kit.html#starter-kits","text":"","title":"Starter Kits"},{"location":"reference/tasks/placeholder.html","text":"Task Placeholder page \u00b6 For each task the content should be: Description \u00b6 What is the purpose of the task Requirements \u00b6 For the task to complete successfully what are the assumptions, prereqs, ..... Source \u00b6 Link to github repo source","title":"ibm-tag-release"},{"location":"reference/tasks/placeholder.html#task-placeholder-page","text":"For each task the content should be:","title":"Task Placeholder page"},{"location":"reference/tasks/placeholder.html#description","text":"What is the purpose of the task","title":"Description"},{"location":"reference/tasks/placeholder.html#requirements","text":"For the task to complete successfully what are the assumptions, prereqs, .....","title":"Requirements"},{"location":"reference/tasks/placeholder.html#source","text":"Link to github repo source","title":"Source"},{"location":"reference/terraform/terraform.html","text":"Cloud-Native Toolkit Reference \u00b6 Todo Add a summary of the reference content here","title":"Terraform modules"},{"location":"reference/terraform/terraform.html#cloud-native-toolkit-reference","text":"Todo Add a summary of the reference content here","title":"Cloud-Native Toolkit Reference"},{"location":"reference/tools/argocd.html","text":"","title":"ArgoCD"},{"location":"reference/tools/artifactory.html","text":"","title":"Artifactory"},{"location":"reference/tools/dashboard.html","text":"Developer Dashboard \u00b6","title":"Developer Dashboard"},{"location":"reference/tools/dashboard.html#developer-dashboard","text":"","title":"Developer Dashboard"},{"location":"reference/tools/ibm-cloud-container-registry.html","text":"IBM Cloud Container Registry \u00b6","title":"IBM Container Registry"},{"location":"reference/tools/ibm-cloud-container-registry.html#ibm-cloud-container-registry","text":"","title":"IBM Cloud Container Registry"},{"location":"reference/tools/pact.html","text":"","title":"PACT"},{"location":"reference/tools/sonar-qube.html","text":"","title":"SonarQube"},{"location":"reference/tools/tekton.html","text":"","title":"OpenShift Pipelines/Tekton"},{"location":"resources/resources.html","text":"Additional Resources \u00b6 Todo Complete content - take from here What is appropriate? Cloud Native Bootcamp : https://cloudnative101.dev Learning Journey : https://cloudnativetoolkit.dev/overview - need to split our sections? Alternate learning options \u00b6 Workshop : Assets to run a workshop with local git server Additional content \u00b6 Todo Add this content Office Hour replays YouTube channel","title":"Additional resources"},{"location":"resources/resources.html#additional-resources","text":"Todo Complete content - take from here What is appropriate? Cloud Native Bootcamp : https://cloudnative101.dev Learning Journey : https://cloudnativetoolkit.dev/overview - need to split our sections?","title":"Additional Resources"},{"location":"resources/resources.html#alternate-learning-options","text":"Workshop : Assets to run a workshop with local git server","title":"Alternate learning options"},{"location":"resources/resources.html#additional-content","text":"Todo Add this content Office Hour replays YouTube channel","title":"Additional content"},{"location":"resources/ibm-cloud/access-control.html","text":"Todo Migrate content from here","title":"Access control"},{"location":"resources/ibm-cloud/cloud-shell.html","text":"Todo Migrate content from here","title":"Cloud shell"},{"location":"resources/ibm-cloud/ibm-cloud.html","text":"IBM Cloud \u00b6 Todo Add content to include: ICC Access control for resources Cloud Shell","title":"IBM Cloud"},{"location":"resources/ibm-cloud/ibm-cloud.html#ibm-cloud","text":"Todo Add content to include: ICC Access control for resources Cloud Shell","title":"IBM Cloud"},{"location":"resources/ibm-cloud/icc.html","text":"Todo Migrate content from here Is this needed? All covered in setup or additional information needed?","title":"ICC tool"},{"location":"resources/workshop/appmod.html","text":"Application Modernization DevOps, Monolith to Container \u00b6 Click on image below to launch video: This section will cover: Application Modernization DevOps, Monolith to Container Deploy the modernized Customer Order Services application in a WebSphere Liberty container to a Red Hat OpenShift cluster The DevOps process is composed of Continous Integration (CI) with OpenShift Pipelines (Tekton) and Continous Deployment (CD) with GitOps engine ArgoCD Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell An user with cluster-admin (ie kubeadmin) needs to deploy a DB2 instance to be shared by all the users oc new-project db2 oc create -n db2 serviceaccount mysvcacct oc adm policy add-scc-to-user privileged system:serviceaccount:db2:mysvcacct oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-dc.yaml\" oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-service.yaml\" (Optional) Analyze the application using the following guide Modernizing runtimes with Liberty Download Transformation Advisor The results of a Data Collector is already provided provided download AppSrv01.zip Upload the data collection into Transformation Advisor Review the CustomerOrderServicesApp.ear analysis The migration path files have been deployed to git for this lab. Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If you are using an IBM Cloud cluster login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Liberty AppMod (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = appmod-liberty-toolkit GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton \\ ${ GIT_URL } #master \\ --pipeline ibm-appmod-liberty \\ -p scan-image = false \\ -p health-endpoint = / \\ -p java-bin-path = CustomerOrderServicesApp/target Notice above that the Toolkit pipeline CLI plugin accepts pipeline names and parameters The endpoint to check that the external access is possible is / The Java bin path is not located based on the root of the git repository and instead in CustomerOrderServicesApp/target Every application would have custom values you can pass them to the pipeline as parameters. Verity that the Pipeline started using the URL printed by the command This is a good moment to take a break as the pipelines will take a few minutes. Verify that Pipeline Run completed successfully Review the Pipeline Tasks/Stages Test Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link Review the Application in GitOps git reposistory. The pipeline step gitops is pushing the application manifest into the GitOps git repository Open Git Ops from Console link Navigate to project1/qa/project1/appmod-liberty-toolkit Review the Helm Chart for the Application Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-qa-websphere-liberty (ie project1-qa-websphere-liberty) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT}/appmod-liberty-toolkit (ie project1/qa/project1/appmod-liberty-toolkit) Cluster: in-cluster Namespace: ${TOOLKIT_PROJECT}-qa (ie. project1-qa) Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: ${TOOLKIT_PROJECT}-websphere-liberty (ie project1-websphere-liberty) Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Click on the route url from the appmod-liberty-toolkit deployment, or the link on the circle. Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Now the Websphere application is ready, the development teams can make changes to git repository for the application, while the gitops git repository is owned by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"Application modernization"},{"location":"resources/workshop/appmod.html#application-modernization-devops-monolith-to-container","text":"Click on image below to launch video: This section will cover: Application Modernization DevOps, Monolith to Container Deploy the modernized Customer Order Services application in a WebSphere Liberty container to a Red Hat OpenShift cluster The DevOps process is composed of Continous Integration (CI) with OpenShift Pipelines (Tekton) and Continous Deployment (CD) with GitOps engine ArgoCD Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell An user with cluster-admin (ie kubeadmin) needs to deploy a DB2 instance to be shared by all the users oc new-project db2 oc create -n db2 serviceaccount mysvcacct oc adm policy add-scc-to-user privileged system:serviceaccount:db2:mysvcacct oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-dc.yaml\" oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-service.yaml\" (Optional) Analyze the application using the following guide Modernizing runtimes with Liberty Download Transformation Advisor The results of a Data Collector is already provided provided download AppSrv01.zip Upload the data collection into Transformation Advisor Review the CustomerOrderServicesApp.ear analysis The migration path files have been deployed to git for this lab. Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If you are using an IBM Cloud cluster login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Liberty AppMod (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = appmod-liberty-toolkit GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton \\ ${ GIT_URL } #master \\ --pipeline ibm-appmod-liberty \\ -p scan-image = false \\ -p health-endpoint = / \\ -p java-bin-path = CustomerOrderServicesApp/target Notice above that the Toolkit pipeline CLI plugin accepts pipeline names and parameters The endpoint to check that the external access is possible is / The Java bin path is not located based on the root of the git repository and instead in CustomerOrderServicesApp/target Every application would have custom values you can pass them to the pipeline as parameters. Verity that the Pipeline started using the URL printed by the command This is a good moment to take a break as the pipelines will take a few minutes. Verify that Pipeline Run completed successfully Review the Pipeline Tasks/Stages Test Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link Review the Application in GitOps git reposistory. The pipeline step gitops is pushing the application manifest into the GitOps git repository Open Git Ops from Console link Navigate to project1/qa/project1/appmod-liberty-toolkit Review the Helm Chart for the Application Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-qa-websphere-liberty (ie project1-qa-websphere-liberty) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT}/appmod-liberty-toolkit (ie project1/qa/project1/appmod-liberty-toolkit) Cluster: in-cluster Namespace: ${TOOLKIT_PROJECT}-qa (ie. project1-qa) Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: ${TOOLKIT_PROJECT}-websphere-liberty (ie project1-websphere-liberty) Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Click on the route url from the appmod-liberty-toolkit deployment, or the link on the circle. Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Now the Websphere application is ready, the development teams can make changes to git repository for the application, while the gitops git repository is owned by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"Application Modernization DevOps, Monolith to Container"},{"location":"resources/workshop/cd.html","text":"Todo Reformat content import Globals from 'gatsby-theme-carbon/src/templates/Globals'; Promote an Application using CD with GitOps and ArgoCD Prerequisites Complete lab Deploy an Application using CI Pipelines with Tekton . Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Verify Application is deployed in QA Select ArgoCD from the Console Link and login using OpenShift login Filter Applications by name ${TOOLKIT_PROJECT}-qa (ie project1-qa) Select the application master-qa-${TOOLKIT_PROJECT}-app (ie master-qa-project1-app) Verify Application is running in the QA namespace corresponding to your username ${TOOLKIT_PROJECT}-qa Select Developer perspective, select project ${TOOLKIT_PROJECT}-qa and then select Topology from the Console and see the application running Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL cd gitops Review the qa and staging directory in the git repository ls -l qa/ ls -l staging/ Promote the application from QA to STAGING by copying the app manifest files using git git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cp -a qa/ ${ TOOLKIT_PROJECT } / staging/ ${ TOOLKIT_PROJECT } / git add . git commit -m \"Promote Application from QA to STAGING environment for $TOOLKIT_PROJECT \" git push -u origin master Verify Application is deployed in STAGING Select ArgoCD from the Console Link and login using OpenShift login Filter Applications by namespace ${TOOLKIT_PROJECT}-staging (ie project1-staging) Select the application master-staging-${TOOLKIT_PROJECT}-app (ie master-staging-project1-app) Click Refresh Verify Application is running in the STAGING namespace corresponding to your username ${TOOLKIT_PROJECT}-qa Select Developer perspective, select project ${TOOLKIT_PROJECT}-staging and then select Topology from the Console and see the application running Propose a change for the Application in STAGING Update the replica count and create a new git branch in remote repo cat > staging/ ${ TOOLKIT_PROJECT } /app/values.yaml <<EOF global: {} app: replicaCount: 2 EOF git diff git add . git checkout -b ${ TOOLKIT_PROJECT } -pr1 git commit -m \"Update Application in ${ TOOLKIT_PROJECT } -staging namespace\" git push -u origin ${ TOOLKIT_PROJECT } -pr1 Open Git Ops from Console Link Select toolkit/gitops git repository Create a Pull Request Select Pull Request Click New Pull Request Select from compare dropdown the branch ${TOOLKIT_PROJECT}-pr1 Enter a title like Update replica count for app in namespace $TOOLKIT_PROJECT Enter a Comment like We need more instances business is growing Yay! click Create Pull Request Review the PR follow the change management process established by your team. Click Merge Pull Request Click Delete Branch Review that application is scales out Review in ArgoCD UI, it takes about 4 minutes to sync, you can click Refresh Review in OpenShift Console, click the Deployment circle details shows 2 Pods. Congratulations you finished this activity, continue with the lab Deploy a 3 tier Microservice using React, Node.js, and Java","title":"Continuous delivery"},{"location":"resources/workshop/ci.html","text":"Todo Reformat content import Globals from 'gatsby-theme-carbon/src/templates/Globals'; Deploy an Application using CI Pipelines with Tekton Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 or userx with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on username id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project as prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork application template git repo Open Developer Dashboard from the OpenShift Console Select Starter Kits Select One in our case Go Gin Microservice Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) IMPORTANT : Rename Repository Name to app Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / $TOOLKIT_USERNAME /app echo GIT_URL = ${ GIT_URL } Clone the git repository and change directory cd $HOME git clone $GIT_URL cd app Create a pipeline for the application oc pipeline --tekton Use down/up arrow and select ibm-golang Hit Enter to enable image scanning Open the url to see the pipeline running in the OpenShift Console Verify that Pipeline Run completed successfully Review the Pipeline Tasks/Stages Test Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link Select Developer perspective, select project $TOOLKIT_PROJECT-qa and then select Topology from the Console and verify the application running Open the application route url and try out the application using the swagger UI Make a change to the application in the git repository and see the pipeline running again from the Console. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" echo \"A change to trigger a new PipelineRun $( date ) \" >> README.md git add . git commit -m \"update readme\" git push -u origin master Verify that change in Git Server and Git WebHook Open Git Dev from Console Link Navigate to user app git repository Review the recent commit Review the webhook recent delivery Verify that a new Pipeline starts successfully Verify that the App manifests are being updated in the gitops repo in the git account toolkit under the qa directory. Open Git Ops from Console Link Select toolkit/gitops git repository Congratulations you finished this lab, continue with lab Promote an Application using CD with GitOps and ArgoCD","title":"Continuous integration"},{"location":"resources/workshop/inventory.html","text":"Todo Reformat content import Globals from 'gatsby-theme-carbon/src/templates/Globals'; Deploy a 3 tier Microservice using React, Node.js, and Java Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT-dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory Service (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-svc-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${GIT_URL}#master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application TypeScript Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory BFF (TypeScript) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-bff-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${GIT_URL}#master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application React Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory UI (React) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-ui-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${GIT_URL}#master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL gitops-inventory cd gitops-inventory Review the qa directory in the git repository, the directory might be empty if the 3 pipelines are not done yet. ls -l qa/ ${ TOOLKIT_PROJECT } / Review the qa directory in the git repository again ls -l qa/ ${ TOOLKIT_PROJECT } / You should see 3 directories inventory-management-bff-solution/ inventory-management-svc-solution/ inventory-management-ui-solution/ Note :If you don't see the directories, this is a good time for a coffee break of 15 minutes until all 3 Pipeline Runs are done. Once the Pipeline Runs are done, try to list the directories again. Each directory contains their corresponding yaml manifest files (ie Helm Chart) ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-bff-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-svc-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-ui-solution Promote the application to QA using git by creating a manifest yaml (ie Helm Chart) that leverage the Cloud Native Toolkit chart argocd-config to automate the creation of multiple ArgoCD Applications. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cat > qa/ ${ TOOLKIT_PROJECT } /Chart.yaml <<EOF apiVersion: v2 version: 1.0.0 name: project-config-helm description: Chart to configure ArgoCD with the inventory application dependencies: - name: argocd-config version: 0.16.0 repository: https://ibm-garage-cloud.github.io/toolkit-charts EOF cat > qa/ ${ TOOLKIT_PROJECT } /values.yaml <<EOF global: {} argocd-config: repoUrl: \"http://gogs.tools:3000/toolkit/gitops.git\" project: inventory-qa applicationTargets: - targetRevision: master createNamespace: true targetNamespace: ${TOOLKIT_PROJECT}-qa applications: - name: qa-${TOOLKIT_PROJECT}-inventory-svc path: qa/${TOOLKIT_PROJECT}/inventory-management-svc-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-bff path: qa/${TOOLKIT_PROJECT}/inventory-management-bff-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-ui path: qa/${TOOLKIT_PROJECT}/inventory-management-ui-solution type: helm EOF cat qa/ ${ TOOLKIT_PROJECT } /values.yaml git add . git commit -m \"Add inventory application to gitops for project ${ TOOLKIT_PROJECT } \" git push -u origin master Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-inventory (ie project1-inventory) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT} (ie qa/project1) Cluster: in-cluster Namespace: tools Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: inventory-management-svc-solution Review Application: inventory-management-bff-solution Review Application: inventory-management-ui-solution Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Open the Application from the JavaScript UI and make sure the stocks show up in the browser. Click on the route url on from the ui deployment, or the link on the circle. Now the Microservices application is ready for the development teams, in some cases each team will own and work with the git repository for the microservices, while the gitops git repository is own by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"3-tier application"},{"location":"resources/workshop/setup.html","text":"Setup Workshop Environment \u00b6 Provides the steps to install the Cloud-Native Toolkit and setting up the Cloud-Native Toolkit Workshop hands on labs. Click on image below to launch video: 1. Create OpenShift Cluster \u00b6 Create an OpenShift Cluster for example: The 8 hours free Cluster on IBM Open Labs select lab 6 Bring Your Own Application Deploy a Cluster on IBM Cloud VPC2 using the Toolkit On other Clouds using docs from cloudnativetoolkit.dev/multi-cloud IBM internal DTE Infrastructure access via IBM VPN or IBM CSPLAB 2. Install IBM Cloud Native Toolkit \u00b6 Use one of the install options for example the Quick Install curl -sfL get.cloudnativetoolkit.dev | sh - 3. Setup Workshop \u00b6 Install the foundation for the workshops curl -sfL workshop.cloudnativetoolkit.dev | sh - Note The username and password for Git Admin is toolkit toolkit 4. (Optional) Auto configure Terminal Shell \u00b6 You can use IBM Cloud Shell , the OpenLabs Shell or your local workstation. More details in Toolkit Dev Setup and Toolkit CLI . Run the following command on Cloud, Linux or MacOS shell: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc Be sure to follow the instructions provided to enable the changes in the current terminal session.","title":"Setup"},{"location":"resources/workshop/setup.html#setup-workshop-environment","text":"Provides the steps to install the Cloud-Native Toolkit and setting up the Cloud-Native Toolkit Workshop hands on labs. Click on image below to launch video:","title":"Setup Workshop Environment"},{"location":"resources/workshop/setup.html#1-create-openshift-cluster","text":"Create an OpenShift Cluster for example: The 8 hours free Cluster on IBM Open Labs select lab 6 Bring Your Own Application Deploy a Cluster on IBM Cloud VPC2 using the Toolkit On other Clouds using docs from cloudnativetoolkit.dev/multi-cloud IBM internal DTE Infrastructure access via IBM VPN or IBM CSPLAB","title":"1. Create OpenShift Cluster"},{"location":"resources/workshop/setup.html#2-install-ibm-cloud-native-toolkit","text":"Use one of the install options for example the Quick Install curl -sfL get.cloudnativetoolkit.dev | sh -","title":"2. Install IBM Cloud Native Toolkit"},{"location":"resources/workshop/setup.html#3-setup-workshop","text":"Install the foundation for the workshops curl -sfL workshop.cloudnativetoolkit.dev | sh - Note The username and password for Git Admin is toolkit toolkit","title":"3. Setup Workshop"},{"location":"resources/workshop/setup.html#4-optional-auto-configure-terminal-shell","text":"You can use IBM Cloud Shell , the OpenLabs Shell or your local workstation. More details in Toolkit Dev Setup and Toolkit CLI . Run the following command on Cloud, Linux or MacOS shell: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc Be sure to follow the instructions provided to enable the changes in the current terminal session.","title":"4. (Optional) Auto configure Terminal Shell"},{"location":"resources/workshop/workshop.html","text":"IBM Cloud Native Toolkit Workshop \u00b6 The Workshop is design to provide a quick way to try the methodology leveraging the tools that the Toolkit integrates. Agenda \u00b6 Setup Workshop Environment ( Admin only students should jump to part 2 ) Deploy an Application using CI Pipelines with Tekton Promote an Application using CD with GitOps and ArgoCD Deploy a 3 tier Microservice using React, Node.js, and Java App Modernization with modern DevOps","title":"Workshop overview"},{"location":"resources/workshop/workshop.html#ibm-cloud-native-toolkit-workshop","text":"The Workshop is design to provide a quick way to try the methodology leveraging the tools that the Toolkit integrates.","title":"IBM Cloud Native Toolkit Workshop"},{"location":"resources/workshop/workshop.html#agenda","text":"Setup Workshop Environment ( Admin only students should jump to part 2 ) Deploy an Application using CI Pipelines with Tekton Promote an Application using CD with GitOps and ArgoCD Deploy a 3 tier Microservice using React, Node.js, and Java App Modernization with modern DevOps","title":"Agenda"},{"location":"setup/fast-start.html","text":"Fast-Start Install \u00b6 Todo This page is only partially complete This section will guide you through installing the Cloud-Native Toolkit suitable for learning how to use the toolkit. Fast-start installation does not install the base Kubernetes cluster. You need to provide the cluster before starting the toolkit installation. Choosing a Kubernetes cluster option \u00b6 The toolkit can be installed over a standard Kubernetes or Red Hat OpenShift cluster. The following options are supported by the toolkit: Todo State any configuration/setup requirements in the tabs for each environment. Such as having a default storage class defined, IAM Admin permissions, etc... OpenShift on IBM Cloud Red Hat OpenShift running on IBM Cloud \u00b6 A Red Hat OpenShift cluster is the recommended production development environment. It provides enhanced developer experience and tooling over a standard Kubernetes cluster as well as additional features, such as enhanced security for production workloads. You need an active IBM Cloud account, with billing enabled, as this option will incur costs Learn on recommended production environment Incurs costs Enhanced developer experience and cluster security over standard Kubernetes No local resources needed to run cluster Kubernetes on IBM Cloud Kubernetes running on IBM Cloud \u00b6 Managed Kubernetes environment Incurs costs No local resources needed to run cluster Code Ready Containers Code Ready Containers \u00b6 Run locally on laptop or workstation no runtime costs Need 16GB memory or greater No remote access from public internet services, such as github cluster access only from host system by default - no remote access to cluster over network Local OpenShift Local OpenShift / OKD \u00b6 Run on local hardware need your own OpenShift licenses (OKD is a sibling project that does not need OpenShift licences) you need to Open Labs cluster Open Labs cluster \u00b6 No local resources needed to run cluster No runtime costs Limited time cluster (6 hours) Obtaining your Kubernetes Cluster \u00b6 Todo Ensure this section also covers installing the CLI for the cluster (oc or kubectl) Select the option you want for your cluster, then follow the instructions. OpenShift on IBM Cloud Red Hat OpenShift running on IBM Cloud \u00b6 Todo Add instructions here Kubernetes on IBM Cloud Kubernetes running on IBM Cloud \u00b6 Todo Add instructions here Code Ready Containers Code Ready Containers \u00b6 Todo Add instructions here Local OpenShift Local OpenShift / OKD \u00b6 Todo Add instructions here Open Labs cluster Open Labs cluster \u00b6 Todo Add instructions here Installing the toolkit \u00b6 To install the toolkit perform the following steps: In a command or terminal window ensure you are logged onto your cluster ( oc login or kubectl login ) with an admin account, that can create new namespaces on the cluster and setup RBAC security. Run the following command (choose your operating system): Linux / MacOS curl -sfL get.cloudnativetoolkit.dev | sh - Windows Todo Does this work for both CMD and Power shell? This uses oc - are we using that for both Kubernetes and OpenShift? oc create -f https : // raw . githubusercontent . com / cloud-native-toolkit / ibm-garage-iteration-zero / master / install / install-ibm -toolkit . yaml sleep 5 oc wait pod -l job-name = ibm-toolkit - -for = condition = Ready -n default oc logs job / ibm-toolkit -f -n default","title":"Fast Start install"},{"location":"setup/fast-start.html#fast-start-install","text":"Todo This page is only partially complete This section will guide you through installing the Cloud-Native Toolkit suitable for learning how to use the toolkit. Fast-start installation does not install the base Kubernetes cluster. You need to provide the cluster before starting the toolkit installation.","title":"Fast-Start Install"},{"location":"setup/fast-start.html#choosing-a-kubernetes-cluster-option","text":"The toolkit can be installed over a standard Kubernetes or Red Hat OpenShift cluster. The following options are supported by the toolkit: Todo State any configuration/setup requirements in the tabs for each environment. Such as having a default storage class defined, IAM Admin permissions, etc... OpenShift on IBM Cloud","title":"Choosing a Kubernetes cluster option"},{"location":"setup/fast-start.html#obtaining-your-kubernetes-cluster","text":"Todo Ensure this section also covers installing the CLI for the cluster (oc or kubectl) Select the option you want for your cluster, then follow the instructions. OpenShift on IBM Cloud","title":"Obtaining your Kubernetes Cluster"},{"location":"setup/fast-start.html#installing-the-toolkit","text":"To install the toolkit perform the following steps: In a command or terminal window ensure you are logged onto your cluster ( oc login or kubectl login ) with an admin account, that can create new namespaces on the cluster and setup RBAC security. Run the following command (choose your operating system): Linux / MacOS curl -sfL get.cloudnativetoolkit.dev | sh - Windows Todo Does this work for both CMD and Power shell? This uses oc - are we using that for both Kubernetes and OpenShift? oc create -f https : // raw . githubusercontent . com / cloud-native-toolkit / ibm-garage-iteration-zero / master / install / install-ibm -toolkit . yaml sleep 5 oc wait pod -l job-name = ibm-toolkit - -for = condition = Ready -n default oc logs job / ibm-toolkit -f -n default","title":"Installing the toolkit"},{"location":"setup/setup-options.html","text":"Installing the Cloud-Native Toolkit \u00b6 Note If you are a developer attending a training event, where the training environment is provided, you should skip this section and move onto the Cloud-Native Toolkit fast start content There is no single way to setup an enterprise development infrastructure for cloud native application development. There are many different tools available to help create Cloud Native applications. You need to find the set of tools and processes to support the needs of the developers and how they work. This can be a time consuming processes, to evaluate available tools and determine how to integrate them into a development environment. The Cloud-Native toolkit is an opinionated set of tools to create a production ready development environment. The toolkit also provides installation options and starter kits to get you up and running quickly. If you don't have access to a Cloud-Native Toolkit then you need to select one of the setup options to install the toolkit before moving onto the learning. Options for installing the toolkit \u00b6 Todo Add a graphic here to summarise the different install options (with indications of when an approach is appropriate) The Cloud-Native toolkit will run on a standard Kubernetes cluster or on the Red Hat OpenShift hybrid cloud platform, which is based on Kubernetes. The Cloud-Native toolkit supports many different options for installing and configuring the toolkit across a number of different Kubernetes based environment, but for the first install it is recommended to follow the fast-start setup. The fast-start installation will create an environment for you to learn about cloud native development. Once you have a better understanding of cloud native development you can then make informed decisions about how you want to do Cloud-Native Development within your production environment and customize the toolkit to meet your requirements. You will find additional installation and toolkit configuration options in the Adopting the toolkit section, but the recommended path for your first installation is the fast-start installation option.","title":"Options for installing up Toolkit"},{"location":"setup/setup-options.html#installing-the-cloud-native-toolkit","text":"Note If you are a developer attending a training event, where the training environment is provided, you should skip this section and move onto the Cloud-Native Toolkit fast start content There is no single way to setup an enterprise development infrastructure for cloud native application development. There are many different tools available to help create Cloud Native applications. You need to find the set of tools and processes to support the needs of the developers and how they work. This can be a time consuming processes, to evaluate available tools and determine how to integrate them into a development environment. The Cloud-Native toolkit is an opinionated set of tools to create a production ready development environment. The toolkit also provides installation options and starter kits to get you up and running quickly. If you don't have access to a Cloud-Native Toolkit then you need to select one of the setup options to install the toolkit before moving onto the learning.","title":"Installing the Cloud-Native Toolkit"},{"location":"setup/setup-options.html#options-for-installing-the-toolkit","text":"Todo Add a graphic here to summarise the different install options (with indications of when an approach is appropriate) The Cloud-Native toolkit will run on a standard Kubernetes cluster or on the Red Hat OpenShift hybrid cloud platform, which is based on Kubernetes. The Cloud-Native toolkit supports many different options for installing and configuring the toolkit across a number of different Kubernetes based environment, but for the first install it is recommended to follow the fast-start setup. The fast-start installation will create an environment for you to learn about cloud native development. Once you have a better understanding of cloud native development you can then make informed decisions about how you want to do Cloud-Native Development within your production environment and customize the toolkit to meet your requirements. You will find additional installation and toolkit configuration options in the Adopting the toolkit section, but the recommended path for your first installation is the fast-start installation option.","title":"Options for installing the toolkit"}]}